{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence (seq2seq) Recurrent Neural Network (RNN) for Time Series Prediction\n",
    "\n",
    "The goal of this project of mine is to bring users to try and experiment with the seq2seq neural network architecture. This is done by solving different simple toy problems about signal prediction. Normally, seq2seq architectures may be used for other more sophisticated purposes than for signal prediction, let's say, language modeling, but this project is an interesting tutorial in order to then get to more complicated stuff. \n",
    "\n",
    "In this project are given 4 exercises of gradually increasing difficulty. I take for granted that the public already have at least knowledge of basic RNNs and how can they be shaped into an encoder and a decoder of the most simple form (without attention). To learn more about RNNs in TensorFlow, you may want to visit this other project of mine about that: https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "\n",
    "The current project is a series of example I have first built in French, but I haven't got the time to generate all the charts anew with proper English text. I have built this project for the practical part of the third hour of a \"master class\" conference that I gave at the WAQ (Web At Quebec) in March 2017: \n",
    "https://webaquebec.org/classes-de-maitre/deep-learning-avec-tensorflow\n",
    "\n",
    "You can find the French, original, version of this project in the French Git branch: https://github.com/guillaume-chevalier/seq2seq-signal-prediction/tree/francais\n",
    "\n",
    "## How to use this \".ipynb\" Python notebook ?\n",
    "\n",
    "Except the fact I made available an \".py\" Python version of this tutorial within the repository, it is more convenient to run the code inside the notebook. The \".py\" code exported feels a bit raw as an exportation. \n",
    "\n",
    "To run the notebook, you must have installed Jupyter Notebook or iPython Notebook. To open the notebook, you must write `jupyter notebook` or `iPython notebook` in command line (from the folder containing the notebook once downloaded, or a parent folder). It is then that the notebook application (IDE) will open in your browser as a local server and it will be possible to open the `.ipynb` notebook file and to run code cells with `CTRL+ENTER` and `SHIFT+ENTER`, it is also possible to restart the kernel and run all cells at once with the menus. Note that this is interesting since it is possible to make that IDE run as hosted on a cloud server with a lot of GPU power while you code through the browser.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Note that the dataset changes in function of the exercice. Most of the time, you will have to edit the neural networks' training parameter to succeed in doing the exercise, but at a certain point, changes in the architecture itself will be asked and required. The datasets used for this exercises are found in `datasets.py`.\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "In theory, it is possible to create a perfect prediction of the signal for this exercise. The neural network's parameters has been set to acceptable values for a first training, so you may pass this exercise by running the code without even a change. Your first training might get predictions like that (in yellow), but it is possible to do a lot better with proper parameters adjustments:\n",
    "\n",
    "<img src=\"images/E1.png\" />\n",
    "\n",
    "Note: the neural network sees only what is to the left of the chart and is trained to predict what is at the right (predictions in yellow). \n",
    "\n",
    "We have 2 time series at once to predict, which are tied together. That means our neural network processes multidimensional data. A simple example would be to receive as an argument the past values of multiple stock market symbols in order to predict the future values of all those symbols with the neural network, which values are evolving together in time. That is what we will do in the exercise 6. \n",
    "\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Here, rather than 2 signals in parallel to predict, we have only one, for simplicity. HOWEVER, this signal is a superposition of two sine waves of varying wavelenght and offset (and restricted to a particular min and max limit of wavelengts). \n",
    "\n",
    "In order to finish this exercise properly, you will need to edit the neural network's hyperparameters. As an example, here is what is possible to achieve as a predction with those better (but still unperfect) training hyperparameters: \n",
    "\n",
    "- `nb_iters = 2500`\n",
    "- `batch_size = 50`\n",
    "- `hidden_dim = 35`\n",
    "<img src=\"images/E2.png\" />\n",
    "\n",
    "Here are predictions achieved with a bigger neural networks with 3 stacked recurrent cells and a width of 500 hidden units for each of those cells: \n",
    "\n",
    "<img src=\"images/E2_1.png\" />\n",
    "\n",
    "<img src=\"images/E2_2.png\" />\n",
    "\n",
    "<img src=\"images/E2_3.png\" />\n",
    "\n",
    "<img src=\"images/E2_4.png\" />\n",
    "\n",
    "Note that it would be possible to obtain better results with a smaller neural network, provided better training hyperparameters and a longer training, adding dropout, and on. \n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "This exercise is similar to the previous one, except that the input data given to the encoder is noisy. The expected output is not noisy. This makes the task a bit harder. Here is a good example of what a training example (and a prediction) could now looks like :\n",
    "\n",
    "<img src=\"images/E3.png\" />\n",
    "\n",
    "Therefore the neural network is brought to denoise the signal to interpret its future smooth values. Here are some example of better predictions on this version of the dataset : \n",
    "\n",
    "<img src=\"images/E3_1.png\" />\n",
    "\n",
    "<img src=\"images/E3_2.png\" />\n",
    "\n",
    "<img src=\"images/E3_3.png\" />\n",
    "\n",
    "<img src=\"images/E3_4.png\" />\n",
    "\n",
    "Similarly as I said for the exercise 2, it would be possible here too to obtain better results. Note that it would also have been possible to ask you to predict to reconstruct the denoised signal from the noisy input (and not predict the future values of it). This would have been called a \"denoising autoencoder\", this type of architecture is also useful for data compression, such as manipulating images. \n",
    "\n",
    "### Exercise 4\n",
    "\n",
    "This exercise is much harder than the previous ones and is built more as a suggestion. It is to predict the future value of the Bitcoin's price. We have here some daily market data of the bitcoin's value, that is, BTC/USD and BTC/EUR. This is not enough to build a good predictor, at least having data precise at the minute level, or second level, would be more interesting. Here is a prediction made on the actual future values, the neural network has not been trained on the future values shown here and this is a legitimate prediction, given a well-enough model trained on the task: \n",
    "\n",
    "<img src=\"images/E5.png\" />\n",
    "\n",
    "Disclaimer: this prediction of the future values was really good and you should not expect predictions to be always that good using as few data as actually (side note: the other prediction charts in this project are all \"average\" except this one). Your task for this exercise is to plug the model on more valuable financial data in order to make more accurate predictions. Let me remind you that I provided the code for the datasets in \"datasets.py\", but that should be replaced for predicting accurately the Bitcoin. \n",
    "\n",
    "It would be possible to improve the input dimensions of your model that accepts (BTC/USD and BTC/EUR). As an example, you could create additionnal input dimensions/streams which could contain meteo data and more financial data, such as the S&P 500, the Dow Jones, and on. Other more creative input data could be sine waves (or other-type-shaped waves such as saw waves or triangles or two signals for `cos` and `sin`) representing the fluctuation of minutes, hours, days, weeks, months, years, moon cycles, and on. This could be combined with a Twitter sentiment analysis about the word \"Bitcoin\" in tweets in order to have another input signal which is more human-based and abstract. Actually, some libraries exists to convert text to a sentiment value, and there would also be the neural network end-to-end approach (but that would be a way more complicated setup). It is also interesting to know where is the bitcoin most used: http://images.google.com/search?tbm=isch&q=bitcoin+heatmap+world\n",
    "\n",
    "With all the above-mentionned examples, it would be possible to have all of this as input features, at every time steps: (BTC/USD, BTC/EUR, Dow_Jones, SP_500, hours, days, weeks, months, years, moons, meteo_USA, meteo_EUROPE, Twitter_sentiment). Finally, there could be those two output features, or more: (BTC/USD, BTC/EUR). \n",
    "\n",
    "This prediction concept can apply to many things, such as meteo prediction and other types of shot-term and mid-term statistical predictions. \n",
    "\n",
    "## To change which exercise you are doing, change the value of the following \"exercise\" variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exercise = 1  # Possible values: 1, 2, 3, or 4.\n",
    "\n",
    "from datasets import generate_x_y_data_v1, generate_x_y_data_v2, generate_x_y_data_v3, generate_x_y_data_v4 \n",
    "\n",
    "# We choose which data function to use below, in function of the exericse. \n",
    "if exercise == 1:\n",
    "    generate_x_y_data = generate_x_y_data_v1\n",
    "if exercise == 2:\n",
    "    generate_x_y_data = generate_x_y_data_v2\n",
    "if exercise == 3:\n",
    "    generate_x_y_data = generate_x_y_data_v3\n",
    "if exercise == 4:  \n",
    "    generate_x_y_data = generate_x_y_data_v4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf  # Version 1.0 or 0.12\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is for the notebook to generate inline matplotlib \n",
    "# charts rather than to open a new window every time: \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network's hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print  (sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the dataset for 3 X and 3 Y training examples : \n",
      "(10, 1, 3)\n",
      "(10, 1, 3)\n",
      "(seq_length, batch_size, output_dim)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exercise = 1  # Possible values: 1, 2, 3, or 4.\n",
    "\n",
    "from datasets import generate_x_y_data_v1, generate_x_y_data_v2, generate_x_y_data_v3, generate_x_y_data_v4 \n",
    "\n",
    "# We choose which data function to use below, in function of the exericse. \n",
    "if exercise == 1:\n",
    "    generate_x_y_data = generate_x_y_data_v1\n",
    "\n",
    "\n",
    "sample_x, sample_y = generate_x_y_data(isTrain=True, batch_size=1)\n",
    "print(\"Dimensions of the dataset for 3 X and 3 Y training examples : \")\n",
    "print(sample_x.shape)\n",
    "print(sample_y.shape)\n",
    "print(\"(seq_length, batch_size, output_dim)\")\n",
    "\n",
    "# Internal neural network parameters\n",
    "seq_length = sample_x.shape[0]  # Time series will have the same past and future (to be predicted) lenght. \n",
    "batch_size = 5  # Low value used for live demo purposes - 100 and 1000 would be possible too, crank that up!\n",
    "\n",
    "output_dim = input_dim = sample_x.shape[-1]  # Output dimension (e.g.: multiple signals at once, tied in time)\n",
    "hidden_dim = 12  # Count of hidden neurons in the recurrent units. \n",
    "layers_stacked_count = 2  # Number of stacked recurrent cells, on the neural depth axis. \n",
    "\n",
    "# Optmizer: \n",
    "learning_rate = 0.007  # Small lr helps not to diverge during training. \n",
    "nb_iters = 150  # How many times we perform a training step (therefore how many times we show a batch). \n",
    "lr_decay = 0.92  # default: 0.9 . Simulated annealing.\n",
    "momentum = 0.5  # default: 0.0 . Momentum technique in weights update\n",
    "lambda_l2_reg = 0.003  # L2 regularization of weights - avoids overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the seq2seq neuronal architecture\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/basic_seq2seq.png\" />\n",
    "\n",
    "Comparatively to what we see in the image, our neural network deals with signal rather than letters. Also, we don't have the feedback mechanism yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow's version : 1.0 (or more)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Backward compatibility for TensorFlow's version 0.12: \n",
    "try:\n",
    "    tf.nn.seq2seq = tf.contrib.legacy_seq2seq\n",
    "    tf.nn.rnn_cell = tf.contrib.rnn\n",
    "    tf.nn.rnn_cell.GRUCell = tf.contrib.rnn.GRUCell\n",
    "    print(\"TensorFlow's version : 1.0 (or more)\")\n",
    "except: \n",
    "    print(\"TensorFlow's version : 0.12\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "# sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "with tf.variable_scope('Seq2seq'):\n",
    "\n",
    "    # Encoder: inputs\n",
    "    enc_inp = [\n",
    "        tf.placeholder(tf.float32, shape=(None, input_dim), name=\"inp_{}\".format(t))\n",
    "           for t in range(seq_length)\n",
    "    ]\n",
    "\n",
    "    # Decoder: expected outputs\n",
    "    expected_sparse_output = [\n",
    "        tf.placeholder(tf.float32, shape=(None, output_dim), name=\"expected_sparse_output_\".format(t))\n",
    "          for t in range(seq_length)\n",
    "    ]\n",
    "    \n",
    "    # Give a \"GO\" token to the decoder. \n",
    "    # Note: we might want to fill the encoder with zeros or its own feedback rather than with \"+ enc_inp[:-1]\"\n",
    "    dec_inp = [ tf.zeros_like(enc_inp[0], dtype=np.float32, name=\"GO\") ] + enc_inp[:-1]\n",
    "\n",
    "    # Create a `layers_stacked_count` of stacked RNNs (GRU cells here). \n",
    "    cells = []\n",
    "    for i in range(layers_stacked_count):\n",
    "        with tf.variable_scope('RNN_{}'.format(i)):\n",
    "            cells.append(tf.nn.rnn_cell.GRUCell(hidden_dim))\n",
    "            # cells.append(tf.nn.rnn_cell.BasicLSTMCell(...))\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    # Here, the encoder and the decoder uses the same cell, HOWEVER,\n",
    "    # the weights aren't shared among the encoder and decoder, we have two\n",
    "    # sets of weights created under the hood according to that function's def. \n",
    "    dec_outputs, dec_memory = tf.nn.seq2seq.basic_rnn_seq2seq(\n",
    "        enc_inp, \n",
    "        dec_inp, \n",
    "        cell\n",
    "    )\n",
    "    \n",
    "    # For reshaping the output dimensions of the seq2seq RNN: \n",
    "    w_out = tf.Variable(tf.random_normal([hidden_dim, output_dim]))\n",
    "    b_out = tf.Variable(tf.random_normal([output_dim]))\n",
    "    \n",
    "    # Final outputs: with linear rescaling for enabling possibly large and unrestricted output values.\n",
    "    output_scale_factor = tf.Variable(1.0, name=\"Output_ScaleFactor\")\n",
    "    \n",
    "    reshaped_outputs = [output_scale_factor*(tf.matmul(i, w_out) + b_out) for i in dec_outputs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loss and optimizer\n",
    "\n",
    "with tf.variable_scope('Loss'):\n",
    "    # L2 loss\n",
    "    output_loss = 0\n",
    "    for _y, _Y in zip(reshaped_outputs, expected_sparse_output):\n",
    "        output_loss += tf.reduce_mean(tf.nn.l2_loss(_y - _Y))\n",
    "                \n",
    "    # L2 regularization (to avoid overfitting and to have a  better generalization capacity)\n",
    "    reg_loss = 0\n",
    "    for tf_var in tf.trainable_variables():\n",
    "        if not (\"Bias\" in tf_var.name or \"Output_\" in tf_var.name):\n",
    "            reg_loss += tf.reduce_mean(tf.nn.l2_loss(tf_var))\n",
    "            \n",
    "    loss = output_loss + lambda_l2_reg * reg_loss\n",
    "\n",
    "with tf.variable_scope('Optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=lr_decay, momentum=momentum)\n",
    "    train_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10714492  1.9369652  -0.02967274]\n",
      " [-0.23263974  2.232553    0.22289598]\n",
      " [-0.21387403  2.0106933  -0.04849446]\n",
      " [ 0.07914394  1.8800364   0.13305533]\n",
      " [ 0.16360141  1.8691401   0.2343384 ]] [[-0.31615248  0.9487084   0.05      ]\n",
      " [ 0.8192923   0.57337606  0.05      ]\n",
      " [ 0.07628017  0.9970864   0.05      ]\n",
      " [-0.7046325   0.70957243  0.05      ]\n",
      " [-0.8206672   0.5714065   0.05      ]]\n",
      "Step 0/150, train loss: 156.17044067382812, \tTEST loss: 43.00661849975586\n",
      "[[ 0.52245194  0.43745917  0.15910539]\n",
      " [ 0.49067086  0.89840984  0.3619839 ]\n",
      " [ 0.54855454  0.5529071   0.25190789]\n",
      " [-0.2808097   0.7604305  -0.42643803]\n",
      " [-0.22183427  0.6265678  -0.49593258]] [[-0.88940936 -0.45711157  0.05      ]\n",
      " [-0.21050152 -0.97759354  0.05      ]\n",
      " [-0.7418114  -0.6706086   0.05      ]\n",
      " [ 0.2739765   0.9617364   0.05      ]\n",
      " [-0.01140328  0.999935    0.05      ]]\n",
      "[[ 9.40770805e-02 -7.79223859e-01 -5.23455262e-01]\n",
      " [ 3.69773060e-01 -1.96577266e-01 -2.02921759e-02]\n",
      " [ 2.42012277e-01  7.24082440e-02  1.31651144e-02]\n",
      " [ 4.36286330e-01 -4.05516475e-01 -5.13657741e-02]\n",
      " [-1.02937706e-01  3.93937945e-01  9.24857086e-05]] [[-0.83806163  0.54557556  0.05      ]\n",
      " [ 0.06934962 -0.9975924   0.05      ]\n",
      " [ 0.47566164 -0.87962836  0.05      ]\n",
      " [-0.24346952 -0.96990854  0.05      ]\n",
      " [ 0.99146324 -0.13038644  0.05      ]]\n",
      "[[ 0.22807892 -0.14925064  0.40130427]\n",
      " [-0.2586252   0.36879674  0.40539712]\n",
      " [ 0.24057493 -0.9001044   0.14854327]\n",
      " [ 0.2902933  -0.3690095   0.36029387]\n",
      " [-0.38199264 -0.22057733  0.0090159 ]] [[ 0.29096597 -0.9567334   0.05      ]\n",
      " [ 0.9602533   0.27913004  0.05      ]\n",
      " [-0.92171484 -0.3878682   0.05      ]\n",
      " [-0.02178396 -0.9997627   0.05      ]\n",
      " [-0.01408664  0.99990076  0.05      ]]\n",
      "[[ 0.17043063 -0.487465    0.37544647]\n",
      " [-0.02982742 -0.6152618   0.21239235]\n",
      " [ 0.13406835  0.12345006  0.5516553 ]\n",
      " [ 0.18637724 -0.14475715  0.48477042]\n",
      " [-0.14254643  0.5784135   0.63152874]] [[-0.52530247 -0.85091555  0.05      ]\n",
      " [-0.9738769  -0.22707656  0.05      ]\n",
      " [ 0.44006327 -0.89796674  0.05      ]\n",
      " [ 0.05321195 -0.99858326  0.05      ]\n",
      " [ 0.99521405 -0.09771902  0.05      ]]\n",
      "[[ 0.24005353 -0.54992247  0.17138065]\n",
      " [ 0.16449063  0.21382296  0.38598552]\n",
      " [ 0.16711284 -0.6485415   0.11860375]\n",
      " [ 0.28237095 -0.2442853   0.26726735]\n",
      " [ 0.04165272 -0.67621166  0.05334695]] [[-0.35003477 -0.9367367   0.05      ]\n",
      " [ 0.8824993  -0.47031376  0.05      ]\n",
      " [-0.6334203  -0.77380794  0.05      ]\n",
      " [ 0.23602039 -0.9717481   0.05      ]\n",
      " [-0.88261634 -0.47009405  0.05      ]]\n",
      "[[ 0.38088804 -0.11607628  0.11347735]\n",
      " [ 0.22616373  0.04727376  0.1338545 ]\n",
      " [-0.09649241 -0.7170397  -0.24690482]\n",
      " [-0.07645039 -0.7379967  -0.24243334]\n",
      " [-0.2985931  -0.19508925 -0.19062571]] [[ 0.92621267 -0.37700143  0.05      ]\n",
      " [ 0.986288    0.1650333   0.05      ]\n",
      " [-0.99987495  0.01581494  0.05      ]\n",
      " [-0.99928546 -0.03779615  0.05      ]\n",
      " [-0.38196018  0.9241788   0.05      ]]\n",
      "[[ 0.5352644  -0.33319727  0.04477401]\n",
      " [-0.11683898  0.13509566  0.01241642]\n",
      " [ 0.4306351  -0.7791176  -0.07285633]\n",
      " [ 0.46356177 -0.73967636 -0.06028257]\n",
      " [ 0.47318545 -0.7251727  -0.05597786]] [[ 0.761475   -0.64819425  0.05      ]\n",
      " [ 0.3250019   0.94571334  0.05      ]\n",
      " [-0.09920838 -0.9950667   0.05      ]\n",
      " [ 0.01917721 -0.9998161   0.05      ]\n",
      " [ 0.05817871 -0.99830616  0.05      ]]\n",
      "[[ 0.44540593 -0.7608229  -0.05943666]\n",
      " [ 0.6143539  -0.310118    0.05354301]\n",
      " [ 0.17393504 -0.80356    -0.09807051]\n",
      " [-0.4678867  -0.04625937 -0.10623004]\n",
      " [ 0.19506116  0.24533984  0.12984517]] [[-0.09024748 -0.99591935  0.05      ]\n",
      " [ 0.7626     -0.6468704   0.05      ]\n",
      " [-0.6029792  -0.7977569   0.05      ]\n",
      " [-0.8120948   0.58352554  0.05      ]\n",
      " [ 0.77374357  0.6334989   0.05      ]]\n",
      "[[-0.45943373  0.39108616  0.02193186]\n",
      " [ 0.22861642  0.35930237  0.14494964]\n",
      " [-0.3765315   0.43571576  0.05011663]\n",
      " [-0.57679904  0.06409203 -0.05675682]\n",
      " [-0.3616752  -0.4191213  -0.08921781]] [[-0.35018313  0.9366813   0.05      ]\n",
      " [ 0.7709388   0.63690925  0.05      ]\n",
      " [-0.1684401   0.9857119   0.05      ]\n",
      " [-0.8630241   0.5051627   0.05      ]\n",
      " [-0.9851847  -0.17149666  0.05      ]]\n",
      "[[ 0.6348463   0.01392596  0.12888378]\n",
      " [ 0.68203765 -0.3445994   0.04971061]\n",
      " [-0.7311949   0.15594202  0.07607547]\n",
      " [ 0.65767807 -0.05564807  0.11333722]\n",
      " [ 0.290944    0.43238112  0.21407467]] [[ 0.9776671  -0.21015958  0.05      ]\n",
      " [ 0.72940534 -0.68408173  0.05      ]\n",
      " [-0.89844525  0.4390856   0.05      ]\n",
      " [ 0.9489027  -0.3155687   0.05      ]\n",
      " [ 0.83331156  0.55280364  0.05      ]]\n",
      "Step 10/150, train loss: 9.54306697845459, \tTEST loss: 7.871214866638184\n",
      "[[ 0.75636643 -0.59657556 -0.05731768]\n",
      " [-0.7440378  -0.0944642   0.04809083]\n",
      " [-0.342356    0.6616181   0.16432613]\n",
      " [ 0.794532   -0.3074486  -0.00337596]\n",
      " [-0.4045426  -0.6250453  -0.02396756]] [[ 0.6133124  -0.7898404   0.05      ]\n",
      " [-0.9909774   0.134029    0.05      ]\n",
      " [-0.0021283   0.99999774  0.05      ]\n",
      " [ 0.8773784  -0.47979906  0.05      ]\n",
      " [-0.8898685  -0.45621714  0.05      ]]\n",
      "[[-0.13960321  0.7737391   0.13748138]\n",
      " [ 0.47095633  0.448442    0.07683052]\n",
      " [ 0.60797757 -0.7693551  -0.1981484 ]\n",
      " [ 0.8402116  -0.21529838 -0.08278158]\n",
      " [-0.47612694 -0.579494   -0.10239492]] [[ 0.263646    0.9646195   0.05      ]\n",
      " [ 0.85496134  0.5186917   0.05      ]\n",
      " [ 0.28101206 -0.9597042   0.05      ]\n",
      " [ 0.9023458  -0.4310129   0.05      ]\n",
      " [-0.8451949  -0.5344582   0.05      ]]\n",
      "[[-0.5271716  -0.5587417  -0.04367491]\n",
      " [-0.35740128 -0.6901787  -0.08687495]\n",
      " [ 0.92303425 -0.17207909 -0.04651039]\n",
      " [-0.22839314  0.89255863  0.2160748 ]\n",
      " [-0.76251453  0.8317042   0.18488778]] [[-0.8318278  -0.55503374  0.05      ]\n",
      " [-0.7150115  -0.6991127   0.05      ]\n",
      " [ 0.9099189  -0.41478616  0.05      ]\n",
      " [ 0.11874515  0.99292475  0.05      ]\n",
      " [-0.4848488   0.87459797  0.05      ]]\n",
      "[[-0.80563164 -0.28769064  0.09128052]\n",
      " [ 0.4967109  -0.87851906 -0.15330271]\n",
      " [-0.70112395 -0.4308354   0.05534495]\n",
      " [-0.34279045  0.91053647  0.25575045]\n",
      " [ 0.7617717  -0.7329836  -0.13743445]] [[-0.96986085 -0.24365939  0.05      ]\n",
      " [ 0.04904543 -0.9987965   0.05      ]\n",
      " [-0.9228038  -0.38527012  0.05      ]\n",
      " [-0.06873928  0.99763465  0.05      ]\n",
      " [ 0.37405142 -0.927408    0.05      ]]\n",
      "[[ 0.30493242 -0.9237788  -0.17056538]\n",
      " [-0.82022756  0.7892008   0.225547  ]\n",
      " [ 0.8740938  -0.62400806 -0.14051646]\n",
      " [ 1.0465307  -0.15655313 -0.05373804]\n",
      " [-0.08542332  0.90096754  0.24882032]] [[-0.11588694 -0.9932624   0.05      ]\n",
      " [-0.580213    0.81446475  0.05      ]\n",
      " [ 0.5373956  -0.8433303   0.05      ]\n",
      " [ 0.92404926 -0.3822735   0.05      ]\n",
      " [ 0.16328593  0.98657876  0.05      ]]\n",
      "[[-1.0136662   0.01228835  0.17690249]\n",
      " [ 0.20907609  0.7420758   0.22187507]\n",
      " [-0.33158424  0.84380645  0.24031122]\n",
      " [ 1.0793066  -0.3158509  -0.05041993]\n",
      " [-0.73489225  0.7396741   0.22182153]] [[-0.9891961   0.14659838  0.05      ]\n",
      " [ 0.3838074   0.92341316  0.05      ]\n",
      " [-0.12087866  0.9926673   0.05      ]\n",
      " [ 0.907684   -0.41965428  0.05      ]\n",
      " [-0.52461433  0.85134     0.05      ]]\n",
      "[[-0.9194658   0.50292385  0.11459189]\n",
      " [ 0.44908535  0.63230884  0.12221771]\n",
      " [ 1.1169009  -0.49511057 -0.11288293]\n",
      " [ 0.9138803  -0.88533986 -0.15543413]\n",
      " [ 1.1137128  -0.11986146 -0.05641472]] [[-0.7832478   0.62170964  0.05      ]\n",
      " [ 0.5435289   0.83939046  0.05      ]\n",
      " [ 0.85033303 -0.5262449   0.05      ]\n",
      " [ 0.50288284 -0.8643546   0.05      ]\n",
      " [ 0.99435526 -0.10610214  0.05      ]]\n",
      "[[-1.0769819   0.28553608  0.09222345]\n",
      " [ 0.7123988   0.5406436   0.05907881]\n",
      " [-0.86835223 -0.46769217  0.04073399]\n",
      " [ 1.0620756  -0.04606078 -0.0569186 ]\n",
      " [ 0.9478557  -0.6844707  -0.13372706]] [[-0.94291365  0.3330373   0.05      ]\n",
      " [ 0.8208202   0.57118666  0.05      ]\n",
      " [-0.9512115  -0.30853963  0.05      ]\n",
      " [ 0.98934865 -0.14556518  0.05      ]\n",
      " [ 0.64385396 -0.7651484   0.05      ]]\n",
      "[[ 0.17941566 -1.074273   -0.17878659]\n",
      " [ 0.84797543 -0.7584878  -0.1598983 ]\n",
      " [-0.99129105 -0.27823174  0.03588122]\n",
      " [-0.22876918 -1.0070832  -0.13579401]\n",
      " [ 0.5539767  -0.9828966  -0.18235154]] [[-0.14353596 -0.9896451   0.05      ]\n",
      " [ 0.5435525  -0.83937514  0.05      ]\n",
      " [-0.97699744 -0.21325113  0.05      ]\n",
      " [-0.48108122 -0.876676    0.05      ]\n",
      " [ 0.21038568 -0.97761846  0.05      ]]\n",
      "[[-0.08818929 -1.0061494  -0.08217411]\n",
      " [ 0.82962596  0.45554087  0.06068652]\n",
      " [-0.72830945 -0.67847604  0.02250515]\n",
      " [ 0.13668968 -1.020814   -0.09752668]\n",
      " [ 0.5964578  -0.88434875 -0.09482288]] [[-0.27568743 -0.9612473   0.05      ]\n",
      " [ 0.920541    0.39064592  0.05      ]\n",
      " [-0.76335275 -0.6459819   0.05      ]\n",
      " [-0.08470958 -0.99640566  0.05      ]\n",
      " [ 0.350161   -0.93668956  0.05      ]]\n",
      "Step 20/150, train loss: 1.8464311361312866, \tTEST loss: 1.0654698610305786\n",
      "[[-1.1656215   0.03273685  0.18873756]\n",
      " [ 0.51779974  0.6970081   0.15222925]\n",
      " [-0.829912    0.9442125   0.1447256 ]\n",
      " [ 0.9820788   0.17452618  0.07267401]\n",
      " [ 0.79724354 -0.9445507  -0.01117762]] [[-0.99949664 -0.03172439  0.05      ]\n",
      " [ 0.5787003   0.8155403   0.05      ]\n",
      " [-0.58968556  0.8076329   0.05      ]\n",
      " [ 0.95907134  0.28316462  0.05      ]\n",
      " [ 0.55075485 -0.8346671   0.05      ]]\n",
      "[[-1.0276654  -0.23501548  0.12806149]\n",
      " [ 1.0276004   0.13786191  0.02112373]\n",
      " [-0.97263616  0.56131303  0.08060484]\n",
      " [-1.0760297   0.02600181  0.1219827 ]\n",
      " [-1.0026289   0.50415426  0.08559399]] [[-0.9918144  -0.12768811  0.05      ]\n",
      " [ 0.93203515  0.36236784  0.05      ]\n",
      " [-0.8373939   0.5465999   0.05      ]\n",
      " [-0.9965495   0.0830004   0.05      ]\n",
      " [-0.86984956  0.49331707  0.05      ]]\n",
      "[[ 0.6288196   0.636241   -0.00156136]\n",
      " [-0.7113966   0.7878486  -0.06070768]\n",
      " [-0.9033656   0.5961845  -0.06039664]\n",
      " [-0.5690578  -0.92220443 -0.08532145]\n",
      " [ 1.0408591   0.23708598 -0.04862095]] [[ 0.54846066  0.83617634  0.05      ]\n",
      " [-0.63124955  0.77557975  0.05      ]\n",
      " [-0.81458974  0.58003753  0.05      ]\n",
      " [-0.74267095 -0.6696565   0.05      ]\n",
      " [ 0.90682137  0.42151508  0.05      ]]\n",
      "[[ 1.0227797   0.38158712  0.08709115]\n",
      " [ 0.5985448  -1.0731888   0.00815626]\n",
      " [-1.0552558  -0.04595442  0.22830598]\n",
      " [-0.5872245  -0.8431817   0.13855591]\n",
      " [-0.7342503   0.99313796  0.1795587 ]] [[ 0.928808    0.37056142  0.05      ]\n",
      " [ 0.26632392 -0.9638836   0.05      ]\n",
      " [-0.9967492  -0.08056697  0.05      ]\n",
      " [-0.7095016  -0.70470375  0.05      ]\n",
      " [-0.5925851   0.80550784  0.05      ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.85766697  0.54552215  0.03973906]\n",
      " [ 1.0088384   0.2591399  -0.00915555]\n",
      " [-0.7721873  -0.7059916   0.06725597]\n",
      " [-0.25506058 -1.0238421  -0.0429623 ]\n",
      " [-1.0266513   0.78012997  0.12884392]] [[ 0.8760872   0.48215276  0.05      ]\n",
      " [ 0.9862526   0.16524458  0.05      ]\n",
      " [-0.750742   -0.6605955   0.05      ]\n",
      " [-0.33450398 -0.9423943   0.05      ]\n",
      " [-0.81936985  0.5732653   0.05      ]]\n",
      "[[ 0.881346   -0.7582969  -0.11093807]\n",
      " [-0.52104294 -1.0091174  -0.02359861]\n",
      " [ 1.0720958  -0.1075213  -0.07210337]\n",
      " [-0.5861277   0.8843853   0.04105606]\n",
      " [ 0.05257235  0.91195565  0.06156435]] [[ 0.71555245 -0.698559    0.05      ]\n",
      " [-0.57473576 -0.81833905  0.05      ]\n",
      " [ 0.9978744  -0.06516714  0.05      ]\n",
      " [-0.48038134  0.87705976  0.05      ]\n",
      " [ 0.09516819  0.9954612   0.05      ]]\n",
      "[[-0.39894697 -1.0211514   0.01495828]\n",
      " [-1.0843388  -0.01895826  0.1373635 ]\n",
      " [-1.0490186  -0.22646172  0.13965581]\n",
      " [-0.8228924   0.74756324  0.07428933]\n",
      " [-0.32509527 -1.0499141   0.00358871]] [[-0.44076067 -0.8976247   0.05      ]\n",
      " [-0.99993575 -0.01133833  0.05      ]\n",
      " [-0.98145854 -0.19167453  0.05      ]\n",
      " [-0.71444553  0.69969106  0.05      ]\n",
      " [-0.3764289  -0.9264455   0.05      ]]\n",
      "[[-1.0127321  -0.22448224  0.09779731]\n",
      " [ 1.0610055  -0.37760535 -0.03021945]\n",
      " [ 1.090047   -0.2273322  -0.02300127]\n",
      " [-0.00404904  0.92326236  0.07305092]\n",
      " [ 1.0352342  -0.45656723 -0.03335109]] [[-0.9858612  -0.16756399  0.05      ]\n",
      " [ 0.9264222  -0.37648627  0.05      ]\n",
      " [ 0.97534716 -0.22067596  0.05      ]\n",
      " [-0.01928701  0.999814    0.05      ]\n",
      " [ 0.8905553  -0.45487496  0.05      ]]\n",
      "[[ 0.6238994  -0.8971446  -0.03197835]\n",
      " [-0.97178537 -0.4141136   0.10149156]\n",
      " [ 0.18693814  0.9059939   0.08477721]\n",
      " [ 0.84580725 -0.6763477  -0.02272842]\n",
      " [-0.7886869  -0.7211041   0.0714352 ]] [[ 0.49753383 -0.8674446   0.05      ]\n",
      " [-0.93590504 -0.3522524   0.05      ]\n",
      " [ 0.21113963  0.9774559   0.05      ]\n",
      " [ 0.72959435 -0.68388015  0.05      ]\n",
      " [-0.7827538  -0.62233144  0.05      ]]\n",
      "[[ 0.65551746 -0.8035103  -0.04468436]\n",
      " [ 0.80905926  0.49568206  0.04279209]\n",
      " [-0.56924653 -0.90431863  0.01978202]\n",
      " [ 0.1890895   0.9432725   0.07618745]\n",
      " [ 0.8562504   0.4259584   0.03459042]] [[ 0.5918358  -0.8060586   0.05      ]\n",
      " [ 0.8463701   0.53259516  0.05      ]\n",
      " [-0.53260505 -0.8463639   0.05      ]\n",
      " [ 0.23814578  0.97122943  0.05      ]\n",
      " [ 0.8908636   0.45427093  0.05      ]]\n",
      "Step 30/150, train loss: 0.7226484417915344, \tTEST loss: 0.7111973166465759\n",
      "[[-0.10483083 -1.0528063   0.01577036]\n",
      " [ 0.52902335  0.7879143   0.10874644]\n",
      " [-1.0529853  -0.14391235  0.17603536]\n",
      " [ 0.9641405  -0.43412575  0.00413872]\n",
      " [ 0.9814987  -0.3886184   0.0054066 ]] [[-0.12219643 -0.9925059   0.05      ]\n",
      " [ 0.51340216  0.85814816  0.05      ]\n",
      " [-0.97559077 -0.2195966   0.05      ]\n",
      " [ 0.89626116 -0.44352672  0.05      ]\n",
      " [ 0.917106   -0.39864355  0.05      ]]\n",
      "[[ 0.3247551  -1.0512941  -0.05426112]\n",
      " [ 0.2753144   0.8891948   0.05352947]\n",
      " [ 0.55867285 -0.94515    -0.05587143]\n",
      " [-0.97970957 -0.3741227   0.1196311 ]\n",
      " [-0.4211566  -1.0002537   0.00830595]] [[ 0.26790944 -0.9634441   0.05      ]\n",
      " [ 0.24821459  0.96870506  0.05      ]\n",
      " [ 0.48578295 -0.87407947  0.05      ]\n",
      " [-0.9232565  -0.38418412  0.05      ]\n",
      " [-0.40830022 -0.9128477   0.05      ]]\n",
      "[[-0.33327007  1.0090834   0.08001368]\n",
      " [-1.0091153   0.4786574   0.12287578]\n",
      " [-1.0685836  -0.01924156  0.171664  ]\n",
      " [-1.0589178   0.28665203  0.14591707]\n",
      " [ 0.17167336 -1.0523324   0.03443   ]] [[-0.31364223  0.9495412   0.05      ]\n",
      " [-0.9331158   0.35957608  0.05      ]\n",
      " [-0.99653834 -0.0831342   0.05      ]\n",
      " [-0.9827348   0.18501976  0.05      ]\n",
      " [ 0.1398913  -0.99016684  0.05      ]]\n",
      "[[-0.9173105  -0.5786035  -0.03781329]\n",
      " [-0.60549796 -1.0400791  -0.08022735]\n",
      " [-0.4490949   0.7852083  -0.052719  ]\n",
      " [-0.73004204 -0.91349024 -0.06298078]\n",
      " [ 0.21268015  0.8329041   0.01557032]] [[-0.9380988  -0.34636775  0.05      ]\n",
      " [-0.6513512  -0.75877637  0.05      ]\n",
      " [-0.47259995  0.8812771   0.05      ]\n",
      " [-0.76665616 -0.6420579   0.05      ]\n",
      " [ 0.15361398  0.9881309   0.05      ]]\n",
      "[[-0.7722126   1.0047973   0.12456568]\n",
      " [-0.01591655 -0.8381143   0.04596746]\n",
      " [ 0.8496525  -0.27615768  0.06666769]\n",
      " [-0.83866954 -0.5611957   0.13910505]\n",
      " [-0.357645    1.1660252   0.13971175]] [[-0.67668575  0.736272    0.05      ]\n",
      " [ 0.07371862 -0.9972791   0.05      ]\n",
      " [ 0.8640882  -0.5033405   0.05      ]\n",
      " [-0.686236   -0.72737896  0.05      ]\n",
      " [-0.3041223   0.95263296  0.05      ]]\n",
      "[[ 0.16935156 -1.3256607   0.16153453]\n",
      " [ 0.9283749   0.29948172  0.16410628]\n",
      " [ 1.140984   -0.24321514  0.12278842]\n",
      " [-0.94809    -0.24298461  0.27429497]\n",
      " [-0.8809614   0.246948    0.22791515]] [[ 0.03909954 -0.99923533  0.05      ]\n",
      " [ 0.7747576   0.6322583   0.05      ]\n",
      " [ 0.9991089   0.04220678  0.05      ]\n",
      " [-0.9950962  -0.09891161  0.05      ]\n",
      " [-0.94030887  0.3403222   0.05      ]]\n",
      "[[ 0.04059941 -0.96405387 -0.25307116]\n",
      " [-0.5205538  -0.89372915 -0.20950983]\n",
      " [-1.0553225   0.43910056 -0.12290592]\n",
      " [ 0.5261093   0.7974359  -0.10784368]\n",
      " [-0.873985    0.7513344  -0.14775425]] [[ 0.13127714 -0.9913457   0.05      ]\n",
      " [-0.38955742 -0.92100215  0.05      ]\n",
      " [-0.9538019   0.3004364   0.05      ]\n",
      " [ 0.5605414   0.82812643  0.05      ]\n",
      " [-0.78836685  0.6152054   0.05      ]]\n",
      "[[-0.69268674  0.7756343   0.16595161]\n",
      " [-0.2779216   0.97886086  0.15550439]\n",
      " [ 1.0057112  -0.32233796  0.12683906]\n",
      " [-0.67999464 -0.841422    0.2866089 ]\n",
      " [ 0.81676924  0.5926737   0.13070263]] [[-0.6709616   0.7414921   0.05      ]\n",
      " [-0.27525097  0.96137244  0.05      ]\n",
      " [ 0.9224103  -0.3862114   0.05      ]\n",
      " [-0.7358857  -0.67710584  0.05      ]\n",
      " [ 0.8154186   0.5788718   0.05      ]]\n",
      "[[-1.4303252e-01  9.3131012e-01  6.8942860e-02]\n",
      " [ 8.8907689e-01 -5.5400836e-01  6.4866737e-02]\n",
      " [ 1.7030686e-01 -1.1302019e+00  8.6846821e-02]\n",
      " [ 6.4211977e-01  6.7567456e-01  7.8814037e-02]\n",
      " [-1.0310856e+00  7.0697215e-04  1.4127570e-01]] [[-0.1308849   0.99139756  0.05      ]\n",
      " [ 0.8361668  -0.5484752   0.05      ]\n",
      " [ 0.1145229  -0.9934206   0.05      ]\n",
      " [ 0.6646418   0.74716216  0.05      ]\n",
      " [-0.99721235  0.0746158   0.05      ]]\n",
      "[[ 0.8898245  -0.35321116 -0.03600867]\n",
      " [-1.0205032  -0.4036168   0.07539963]\n",
      " [ 0.92467874  0.17052723 -0.02546792]\n",
      " [ 0.12426837  0.93572754  0.02164912]\n",
      " [ 0.8556505  -0.44048208 -0.0359004 ]] [[ 0.9243517  -0.3815416   0.05      ]\n",
      " [-0.92615324 -0.3771474   0.05      ]\n",
      " [ 0.98700905  0.16066469  0.05      ]\n",
      " [ 0.15563132  0.9878152   0.05      ]\n",
      " [ 0.885607   -0.46443537  0.05      ]]\n",
      "Step 40/150, train loss: 0.7024941444396973, \tTEST loss: 0.5468831658363342\n",
      "[[ 0.9113379   0.34439293  0.10368954]\n",
      " [ 0.9985719   0.05906359  0.08117449]\n",
      " [ 0.62048    -0.8805091   0.05574942]\n",
      " [-0.9886198  -0.41120553  0.15892352]\n",
      " [-0.08487684 -1.1139638   0.05282442]] [[ 0.91003186  0.41453832  0.05      ]\n",
      " [ 0.99511564  0.09871588  0.05      ]\n",
      " [ 0.5909507  -0.8067076   0.05      ]\n",
      " [-0.9306576  -0.36589128  0.05      ]\n",
      " [-0.05761622 -0.9983388   0.05      ]]\n",
      "[[-0.98524076  0.48552397  0.06377015]\n",
      " [ 0.9582184  -0.02926567  0.00775563]\n",
      " [-1.0505015  -0.1764298   0.10329279]\n",
      " [-1.0137463   0.41028842  0.07047185]\n",
      " [ 0.52411866 -0.7809793  -0.00699814]] [[-0.9224739   0.38605943  0.05      ]\n",
      " [ 0.9879104  -0.15502594  0.05      ]\n",
      " [-0.973995   -0.22656961  0.05      ]\n",
      " [-0.9491914   0.31469935  0.05      ]\n",
      " [ 0.5336069  -0.8457326   0.05      ]]\n",
      "[[ 0.02850286 -1.2353613   0.07534848]\n",
      " [-0.9656777   0.063476    0.0977637 ]\n",
      " [-0.78799975 -0.7901219   0.13124837]\n",
      " [ 0.73827904 -0.90754855  0.07504957]\n",
      " [-0.7386398   0.5945846   0.04636648]] [[-0.05360692 -0.9985621   0.05      ]\n",
      " [-0.9834331   0.18127146  0.05      ]\n",
      " [-0.8255445  -0.564337    0.05      ]\n",
      " [ 0.6302973  -0.77635384  0.05      ]\n",
      " [-0.7512703   0.65999466  0.05      ]]\n",
      "[[ 0.01944231 -0.95621026 -0.02681454]\n",
      " [ 0.014132   -0.95709676 -0.02680089]\n",
      " [-0.5757393   0.92966896 -0.00533323]\n",
      " [ 0.26529834 -0.8836993  -0.02379697]\n",
      " [ 0.07045534  1.0080391   0.03635487]] [[ 0.13392857 -0.990991    0.05      ]\n",
      " [ 0.12899847 -0.9916448   0.05      ]\n",
      " [-0.524495    0.85141355  0.05      ]\n",
      " [ 0.36133495 -0.9324361   0.05      ]\n",
      " [ 0.11453239  0.9934195   0.05      ]]\n",
      "[[ 0.6246911  -0.9149436   0.10345834]\n",
      " [-0.8101672  -0.72188866  0.17754896]\n",
      " [ 0.6222881   0.6713482   0.1396767 ]\n",
      " [-0.60801333 -0.9227325   0.14974208]\n",
      " [ 1.0163299  -0.11964034  0.09503917]] [[ 0.605278   -0.79601413  0.05      ]\n",
      " [-0.73749286 -0.67535496  0.05      ]\n",
      " [ 0.5712301   0.82079     0.05      ]\n",
      " [-0.53284967 -0.84620994  0.05      ]\n",
      " [ 0.9997429  -0.02267223  0.05      ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2309146   0.96033007  0.07142384]\n",
      " [-0.888791    0.6195763   0.08187491]\n",
      " [ 0.8779136   0.46308756  0.02045865]\n",
      " [-0.8551056   0.6670313   0.07707762]\n",
      " [-0.84169513 -0.6322582   0.10688677]] [[ 0.19488227  0.9808266   0.05      ]\n",
      " [-0.86446357  0.5026955   0.05      ]\n",
      " [ 0.877972    0.478712    0.05      ]\n",
      " [-0.8343217   0.55127794  0.05      ]\n",
      " [-0.7601123  -0.6497918   0.05      ]]\n",
      "[[ 0.5555098  -1.002906   -0.01187197]\n",
      " [ 0.72448516  0.64153636  0.02958235]\n",
      " [ 0.9420315  -0.5364156  -0.02135513]\n",
      " [ 0.8237057   0.53662115  0.01629649]\n",
      " [ 0.99422765  0.21749137 -0.01441174]] [[ 0.46406668 -0.8858003   0.05      ]\n",
      " [ 0.709055    0.70515317  0.05      ]\n",
      " [ 0.86067027 -0.50916266  0.05      ]\n",
      " [ 0.81085694  0.5852444   0.05      ]\n",
      " [ 0.9737284   0.2277124   0.05      ]]\n",
      "[[-0.35109654  0.97215694  0.17113562]\n",
      " [ 0.3237962   0.96021855  0.20024401]\n",
      " [ 0.7295643   0.6930138   0.17121103]\n",
      " [ 0.92874146  0.35516566  0.13556427]\n",
      " [ 0.70617557  0.7177899   0.17432305]] [[-0.2938554   0.9558499   0.05      ]\n",
      " [ 0.38276237  0.92384684  0.05      ]\n",
      " [ 0.79285276  0.6094132   0.05      ]\n",
      " [ 0.9727149   0.23200372  0.05      ]\n",
      " [ 0.7699134   0.6381484   0.05      ]]\n",
      "[[ 1.0003033  -0.1480011  -0.02416239]\n",
      " [-0.62839377 -0.9397063  -0.04054583]\n",
      " [-0.8820428   0.26797208 -0.10160393]\n",
      " [-0.8467788  -0.6519739  -0.0320154 ]\n",
      " [ 0.9692548  -0.3551792  -0.01865554]] [[ 0.9920485  -0.1258563   0.05      ]\n",
      " [-0.64487475 -0.7642883   0.05      ]\n",
      " [-0.92390895  0.38261247  0.05      ]\n",
      " [-0.8736966  -0.48647115  0.05      ]\n",
      " [ 0.94318354 -0.33227214  0.05      ]]\n",
      "[[-0.99322367 -0.214726    0.13413441]\n",
      " [ 0.9312531  -0.1615516   0.15409428]\n",
      " [-0.46992317 -0.87650824  0.14682902]\n",
      " [ 0.46003294  0.79519695  0.08462325]\n",
      " [ 0.8570374   0.4085107   0.11729479]] [[-0.9593307  -0.28228465  0.05      ]\n",
      " [ 0.97434705 -0.22505079  0.05      ]\n",
      " [-0.38216463 -0.92409426  0.05      ]\n",
      " [ 0.450346    0.89285415  0.05      ]\n",
      " [ 0.90348506  0.4286196   0.05      ]]\n",
      "Step 50/150, train loss: 0.9410321116447449, \tTEST loss: 0.8028611540794373\n",
      "[[-0.871825    0.52739304  0.11191239]\n",
      " [-0.28379056 -1.0286614   0.13172719]\n",
      " [-0.35243297  0.8801439   0.08574794]\n",
      " [-0.46520332 -0.972702    0.14332984]\n",
      " [-1.0087905   0.22901864  0.15253334]] [[-0.8695053   0.49392366  0.05      ]\n",
      " [-0.16822918 -0.98574793  0.05      ]\n",
      " [-0.39415398  0.91904444  0.05      ]\n",
      " [-0.34349355 -0.93915504  0.05      ]\n",
      " [-0.9822865   0.18738526  0.05      ]]\n",
      "[[ 1.0059885  -0.30469456 -0.04166234]\n",
      " [-0.40317795 -1.1063831  -0.04363438]\n",
      " [-0.38439527  0.7830505  -0.01091023]\n",
      " [-0.60846347 -0.98857474 -0.01878461]\n",
      " [ 0.06062115  0.8644777   0.01065572]] [[ 0.96380526 -0.26660728  0.05      ]\n",
      " [-0.3895426  -0.92100847  0.05      ]\n",
      " [-0.45032465  0.8928649   0.05      ]\n",
      " [-0.5942611  -0.8042722   0.05      ]\n",
      " [-0.01152691  0.99993354  0.05      ]]\n",
      "[[-0.60977495 -0.7783978   0.18377744]\n",
      " [ 0.8744475   0.5594559   0.13167343]\n",
      " [ 0.9612748   0.2042789   0.1199403 ]\n",
      " [ 0.58992046  0.91550577  0.16091277]\n",
      " [-0.23867556  1.1369625   0.1663554 ]] [[-0.496655   -0.86794806  0.05      ]\n",
      " [ 0.9266453   0.37593687  0.05      ]\n",
      " [ 0.9999941  -0.00344172  0.05      ]\n",
      " [ 0.6337605   0.77352935  0.05      ]\n",
      " [-0.21099554  0.977487    0.05      ]]\n",
      "[[-0.9263665   0.07423507  0.04497116]\n",
      " [-0.47098348  0.64095044 -0.03730281]\n",
      " [ 0.74788135 -0.8301261   0.04834455]\n",
      " [-0.00635688 -1.1366771   0.05628509]\n",
      " [ 1.0024529  -0.19798706  0.00734064]] [[-0.9824197   0.18668544  0.05      ]\n",
      " [-0.5620434   0.8271077   0.05      ]\n",
      " [ 0.72244436 -0.691429    0.05      ]\n",
      " [ 0.00576955 -0.9999834   0.05      ]\n",
      " [ 0.9994854  -0.03207758  0.05      ]]\n",
      "[[ 0.7483551  -0.55861896  0.12162273]\n",
      " [ 0.9373957   0.2777228   0.08268411]\n",
      " [ 0.3082534   0.8848065   0.06389251]\n",
      " [ 0.7087462   0.6505089   0.07716163]\n",
      " [ 0.7300762  -0.5796351   0.12192906]] [[ 0.7500254  -0.661409    0.05      ]\n",
      " [ 0.97332656  0.22942418  0.05      ]\n",
      " [ 0.29613274  0.9551468   0.05      ]\n",
      " [ 0.73109746  0.68227303  0.05      ]\n",
      " [ 0.7319907  -0.68131465  0.05      ]]\n",
      "[[ 0.943387    0.17744261  0.12060357]\n",
      " [ 0.8081355  -0.47577676  0.13822229]\n",
      " [-0.71762335 -0.71713656  0.17895888]\n",
      " [-0.53149     0.95576006  0.12638777]\n",
      " [ 0.2143766   0.95997286  0.14249936]] [[ 0.98972726  0.14296831  0.05      ]\n",
      " [ 0.8456848  -0.5336827   0.05      ]\n",
      " [-0.5510582  -0.8344668   0.05      ]\n",
      " [-0.50606346  0.86249626  0.05      ]\n",
      " [ 0.2181067   0.9759249   0.05      ]]\n",
      "[[ 0.93646574 -0.48298246 -0.02932993]\n",
      " [-0.46690768  0.7917522  -0.02487711]\n",
      " [ 0.06016552 -1.1349688  -0.02131641]\n",
      " [-0.8301838   0.5325231  -0.00351284]\n",
      " [ 0.933477    0.17036991 -0.03732903]] [[ 0.9347564  -0.35528922  0.05      ]\n",
      " [-0.4910448   0.87113434  0.05      ]\n",
      " [ 0.11421537 -0.993456    0.05      ]\n",
      " [-0.827741    0.5611103   0.05      ]\n",
      " [ 0.9453223   0.32613763  0.05      ]]\n",
      "[[-0.14960092 -0.98372185  0.10703575]\n",
      " [-1.0202484   0.14559284  0.17216834]\n",
      " [ 0.87829995 -0.43369755  0.08766595]\n",
      " [-0.9573342   0.46858326  0.14013663]\n",
      " [-0.6564108   0.886753    0.09682734]] [[-0.14316574 -0.9896987   0.05      ]\n",
      " [-0.99830604  0.05818081  0.05      ]\n",
      " [ 0.8342762  -0.5513467   0.05      ]\n",
      " [-0.9336434   0.35820386  0.05      ]\n",
      " [-0.63669616  0.77111477  0.05      ]]\n",
      "[[-0.2309754   0.95343864  0.07854337]\n",
      " [-0.5769367   0.818403    0.05471808]\n",
      " [-0.5358546  -1.0417747   0.04628924]\n",
      " [-0.24625638 -1.1533722   0.03310563]\n",
      " [ 0.4796433  -1.0186772   0.05624253]] [[-0.22480199  0.97440445  0.05      ]\n",
      " [-0.55979186  0.82863325  0.05      ]\n",
      " [-0.49212772 -0.87052304  0.05      ]\n",
      " [-0.21300761 -0.97705054  0.05      ]\n",
      " [ 0.45859265 -0.8886466   0.05      ]]\n",
      "[[ 0.825373   -0.33672628  0.04873912]\n",
      " [ 0.6903121  -0.5647695   0.04293982]\n",
      " [ 0.05826164  1.0252327   0.08717638]\n",
      " [ 0.799268    0.54172224  0.07252289]\n",
      " [ 0.80117846  0.538228    0.07230526]] [[ 0.89513445 -0.44579622  0.05      ]\n",
      " [ 0.7585684  -0.65159345  0.05      ]\n",
      " [ 0.09308358  0.9956583   0.05      ]\n",
      " [ 0.88933367  0.4572588   0.05      ]\n",
      " [ 0.89134425  0.45332706  0.05      ]]\n",
      "Step 60/150, train loss: 0.7254497408866882, \tTEST loss: 0.8918393850326538\n",
      "[[-0.1309355  -1.1230141   0.0592087 ]\n",
      " [ 0.66125184 -0.8867067   0.07340001]\n",
      " [-0.86416346 -0.5865264   0.13773851]\n",
      " [-0.5217229   0.6000982   0.10734539]\n",
      " [ 0.6073714   0.5968043   0.13163723]] [[-0.07087112 -0.99748546  0.05      ]\n",
      " [ 0.6719761  -0.74057287  0.05      ]\n",
      " [-0.8730155  -0.4876924   0.05      ]\n",
      " [-0.654592    0.7559824   0.05      ]\n",
      " [ 0.5157175   0.8567587   0.05      ]]\n",
      "[[ 9.3758929e-01  3.6598852e-01  3.8457010e-02]\n",
      " [-4.6208116e-01  9.3156588e-01  2.4408525e-02]\n",
      " [-7.7155012e-01 -6.0288161e-01  4.0667817e-02]\n",
      " [-5.5170911e-03 -8.5394275e-01 -7.1712164e-04]\n",
      " [-1.0219822e+00  2.1069014e-01  6.0475554e-02]] [[ 0.96012384  0.2795751   0.05      ]\n",
      " [-0.50034195  0.8658279   0.05      ]\n",
      " [-0.6546418  -0.75593925  0.05      ]\n",
      " [ 0.10257122 -0.99472564  0.05      ]\n",
      " [-0.99746084  0.07121729  0.05      ]]\n",
      "[[-0.4336515  -1.1146872   0.18959484]\n",
      " [ 1.0576216   0.10229743  0.13277933]\n",
      " [-0.8795342  -0.60436773  0.20496887]\n",
      " [-0.84244126 -0.68232477  0.20552029]\n",
      " [-0.9702082  -0.28359687  0.19142887]] [[-0.47051302 -0.88239306  0.05      ]\n",
      " [ 0.98177534  0.19004521  0.05      ]\n",
      " [-0.89993614 -0.4360217   0.05      ]\n",
      " [-0.8635376  -0.5042844   0.05      ]\n",
      " [-0.98779744 -0.15574408  0.05      ]]\n",
      "[[ 0.50684965 -0.6416933  -0.10727347]\n",
      " [-1.1363539   0.13291474 -0.08313689]\n",
      " [-0.30765387 -0.8587933  -0.16309923]\n",
      " [-0.90132844  0.7411653  -0.09624998]\n",
      " [ 0.89057183  0.11484318 -0.05703646]] [[ 0.6491154  -0.76069     0.05      ]\n",
      " [-0.9994185  -0.03409778  0.05      ]\n",
      " [-0.07514034 -0.99717295  0.05      ]\n",
      " [-0.8035851   0.59518987  0.05      ]\n",
      " [ 0.9999764  -0.00686825  0.05      ]]\n",
      "[[ 0.28736266  0.82825583  0.2022926 ]\n",
      " [ 0.278183   -1.0885643   0.18275894]\n",
      " [ 0.66193676 -0.9346876   0.20001504]\n",
      " [ 0.53818476  0.73563653  0.20682724]\n",
      " [ 0.25021508  0.8364894   0.20079793]] [[ 0.19817556  0.98016655  0.05      ]\n",
      " [ 0.1840102  -0.98292434  0.05      ]\n",
      " [ 0.5429892  -0.8397397   0.05      ]\n",
      " [ 0.46126562  0.8872621   0.05      ]\n",
      " [ 0.15980217  0.98714906  0.05      ]]\n",
      "[[-0.90459293  0.55575585  0.06999788]\n",
      " [ 0.05657154 -1.0527583   0.11563043]\n",
      " [ 0.28632978 -1.0259178   0.11773299]\n",
      " [ 0.9460508   0.19678144  0.05401114]\n",
      " [ 0.8517652  -0.6047739   0.10722048]] [[-0.88357574  0.46828824  0.05      ]\n",
      " [ 0.0472427  -0.9988834   0.05      ]\n",
      " [ 0.26062396 -0.9654404   0.05      ]\n",
      " [ 0.9795635   0.20113516  0.05      ]\n",
      " [ 0.8177761  -0.5755365   0.05      ]]\n",
      "[[ 0.8236456   0.43265817 -0.0424192 ]\n",
      " [-1.0589588   0.19098154  0.04147187]\n",
      " [ 0.55680907 -0.8062105   0.00884428]\n",
      " [-0.2140395  -1.0049264   0.00963128]\n",
      " [-0.29359117 -0.98725593  0.01271671]] [[ 0.94108254  0.33817703  0.05      ]\n",
      " [-0.99650794  0.08349785  0.05      ]\n",
      " [ 0.5892307  -0.8079648   0.05      ]\n",
      " [-0.1424923  -0.9897959   0.05      ]\n",
      " [-0.21885689 -0.97575694  0.05      ]]\n",
      "[[-0.9164803   0.54417115  0.04391421]\n",
      " [-0.6064884   0.9025088   0.02387615]\n",
      " [ 0.5471581  -0.99509937  0.11485121]\n",
      " [ 0.95576096 -0.3982659   0.10468631]\n",
      " [-1.0071884  -0.13753238  0.10717054]] [[-0.8839496   0.46758217  0.05      ]\n",
      " [-0.5703711   0.8213871   0.05      ]\n",
      " [ 0.51058626 -0.85982656  0.05      ]\n",
      " [ 0.93871075 -0.34470585  0.05      ]\n",
      " [-0.98898643 -0.14800623  0.05      ]]\n",
      "[[ 0.89590013 -0.5538207   0.14473929]\n",
      " [ 0.992192   -0.091232    0.12443374]\n",
      " [ 0.55412143  0.7934653   0.10825599]\n",
      " [ 0.72528726 -0.82235885  0.14825495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ 0.7609991  -0.77978295  0.14859918]] [[ 0.8576853  -0.514175    0.05      ]\n",
      " [ 0.995071   -0.09916496  0.05      ]\n",
      " [ 0.58876044  0.8083076   0.05      ]\n",
      " [ 0.67264414 -0.73996615  0.05      ]\n",
      " [ 0.709561   -0.704644    0.05      ]]\n",
      "[[ 0.6589809  -0.77854955  0.05688829]\n",
      " [ 0.65197307 -0.78614783  0.05712326]\n",
      " [-0.9825136  -0.02630697  0.09860554]\n",
      " [ 0.41093826 -0.97488296  0.06023875]\n",
      " [-0.642083   -0.87079924  0.10119371]] [[ 0.6478984  -0.7617268   0.05      ]\n",
      " [ 0.6406556  -0.76782835  0.05      ]\n",
      " [-0.9997245   0.02347098  0.05      ]\n",
      " [ 0.39984474 -0.9165829   0.05      ]\n",
      " [-0.6414476  -0.76716685  0.05      ]]\n",
      "Step 70/150, train loss: 0.5890729427337646, \tTEST loss: 0.6258145570755005\n",
      "[[ 0.775918   -0.60223186  0.05276651]\n",
      " [-0.7644989   0.7147223   0.01984126]\n",
      " [-0.17508341 -1.0902839   0.04915458]\n",
      " [ 0.27953556  0.9079537   0.03946094]\n",
      " [ 0.47951767 -0.9324087   0.05478447]] [[ 0.8482385  -0.5296144   0.05      ]\n",
      " [-0.7362457   0.67671436  0.05      ]\n",
      " [-0.06580469 -0.99783254  0.05      ]\n",
      " [ 0.33282003  0.94299036  0.05      ]\n",
      " [ 0.5579566  -0.82987016  0.05      ]]\n",
      "[[-0.7879452  -0.655011    0.14965826]\n",
      " [ 0.9753097   0.15847236  0.11936387]\n",
      " [-0.9718665  -0.05873124  0.13755767]\n",
      " [ 0.80516    -0.5985085   0.1433065 ]\n",
      " [-0.6055792   0.80372673  0.07447539]] [[-0.79010105 -0.6129766   0.05      ]\n",
      " [ 0.9965119   0.0834514   0.05      ]\n",
      " [-0.9992614  -0.03842762  0.05      ]\n",
      " [ 0.779413   -0.6265105   0.05      ]\n",
      " [-0.62185544  0.7831321   0.05      ]]\n",
      "[[-0.9786024  -0.31643704  0.01325375]\n",
      " [-0.9864675   0.05688415 -0.00357205]\n",
      " [-0.09310961  0.932934   -0.00142755]\n",
      " [ 0.8142827  -0.6087566   0.01463828]\n",
      " [ 0.7514688   0.59786046  0.02252538]] [[-0.9682965  -0.24980366  0.05      ]\n",
      " [-0.99360454  0.11291594  0.05      ]\n",
      " [-0.11474451  0.99339503  0.05      ]\n",
      " [ 0.81925726 -0.5734261   0.05      ]\n",
      " [ 0.7698644   0.6382075   0.05      ]]\n",
      "[[ 0.58650434 -0.78222394  0.19896404]\n",
      " [ 0.17014982  1.0536819   0.18089987]\n",
      " [ 0.5972423  -0.7740902   0.19857062]\n",
      " [ 0.98692244  0.07986824  0.14509511]\n",
      " [-0.8783238  -0.50751615  0.3015492 ]] [[ 0.60638404 -0.7951719   0.05      ]\n",
      " [ 0.15313919  0.98820466  0.05      ]\n",
      " [ 0.61615944 -0.78762144  0.05      ]\n",
      " [ 0.9999545   0.00953413  0.05      ]\n",
      " [-0.80409217 -0.59450465  0.05      ]]\n",
      "[[-0.33438244  0.9146005   0.00339689]\n",
      " [-0.9157002  -0.49956104  0.11539987]\n",
      " [-0.74651784 -0.77694327  0.08281245]\n",
      " [-0.7036608   0.7146652   0.02776493]\n",
      " [-0.11384717  0.9445131  -0.00192177]] [[-0.3381467   0.94109344  0.05      ]\n",
      " [-0.88104606 -0.47303048  0.05      ]\n",
      " [-0.68135005 -0.7319577   0.05      ]\n",
      " [-0.7066316   0.70758164  0.05      ]\n",
      " [-0.11591234  0.99325943  0.05      ]]\n",
      "[[-0.19698466 -1.0950942  -0.11603154]\n",
      " [ 0.58841443 -0.9166244  -0.12535703]\n",
      " [ 0.24699311 -1.081037   -0.12905975]\n",
      " [-0.2629322   0.86766833 -0.07580225]\n",
      " [-0.9417048   0.18060622 -0.02692341]] [[-0.15175647 -0.9884179   0.05      ]\n",
      " [ 0.6031825  -0.79760325  0.05      ]\n",
      " [ 0.27910933 -0.9602593   0.05      ]\n",
      " [-0.26729167  0.96361566  0.05      ]\n",
      " [-0.97086906  0.23961055  0.05      ]]\n",
      "[[ 0.4538377   0.98845905  0.2762465 ]\n",
      " [-0.34377187 -0.8036885   0.33306125]\n",
      " [-0.21966957 -0.8344715   0.33707964]\n",
      " [ 0.16053103  1.0822951   0.25845551]\n",
      " [-0.5260201  -0.7230538   0.32928005]] [[ 0.5297064   0.84818107  0.05      ]\n",
      " [-0.37404662 -0.9274099   0.05      ]\n",
      " [-0.25473696 -0.9670104   0.05      ]\n",
      " [ 0.22780576  0.9737066   0.05      ]\n",
      " [-0.55238086 -0.8335919   0.05      ]]\n",
      "[[ 0.35003257 -1.0004092   0.02666847]\n",
      " [-0.87202764  0.5509125   0.095821  ]\n",
      " [-1.0130477  -0.07269556  0.13169135]\n",
      " [-0.91223603  0.4755801   0.10215937]\n",
      " [ 0.69253486  0.5815464   0.04937568]] [[ 0.46633998 -0.8846056   0.05      ]\n",
      " [-0.8731994   0.48736316  0.05      ]\n",
      " [-0.9917268  -0.12836637  0.05      ]\n",
      " [-0.9122261   0.40968722  0.05      ]\n",
      " [ 0.7310381   0.6823367   0.05      ]]\n",
      "[[ 0.93771356 -0.11654943 -0.06366355]\n",
      " [ 0.08756149 -1.055046   -0.09241424]\n",
      " [-0.6646541   0.686101   -0.01047969]\n",
      " [ 0.67563146  0.6338753  -0.03496082]\n",
      " [ 0.47782162 -0.91073304 -0.07838197]] [[ 0.9962005  -0.08708921  0.05      ]\n",
      " [ 0.16443647 -0.98638767  0.05      ]\n",
      " [-0.70752066  0.70669264  0.05      ]\n",
      " [ 0.7131893   0.7009715   0.05      ]\n",
      " [ 0.5407342  -0.84119356  0.05      ]]\n",
      "[[-0.13934277  1.1247753   0.18950859]\n",
      " [-0.7904789  -0.6925257   0.08927697]\n",
      " [-0.8197774   0.6997158   0.13480194]\n",
      " [-0.29557434 -0.9963268   0.04950708]\n",
      " [-1.0068969   0.13763586  0.13292605]] [[-0.07207827  0.997399    0.05      ]\n",
      " [-0.76556945 -0.6433533   0.05      ]\n",
      " [-0.76945895  0.63869625  0.05      ]\n",
      " [-0.25287762 -0.9674983   0.05      ]\n",
      " [-0.98972476  0.14298579  0.05      ]]\n",
      "Step 80/150, train loss: 0.6783731579780579, \tTEST loss: 2.16007137298584\n",
      "[[-0.22551522  1.1376545   0.22740005]\n",
      " [ 0.86244005  0.5847644   0.27158132]\n",
      " [-0.84870744  0.6923832   0.12333649]\n",
      " [ 0.9524006   0.06024044  0.2486461 ]\n",
      " [-0.8657161   0.66545904  0.12082974]] [[-0.129045    0.9916387   0.05      ]\n",
      " [ 0.9384995   0.3452805   0.05      ]\n",
      " [-0.75802016  0.6522311   0.05      ]\n",
      " [ 0.9905571  -0.13710119  0.05      ]\n",
      " [-0.7762238   0.63045746  0.05      ]]\n",
      "[[-0.7190033   0.02757356 -0.07241394]\n",
      " [ 0.95073426 -0.43047658  0.14298515]\n",
      " [-0.74024916 -0.35204855 -0.0529837 ]\n",
      " [-0.5840849  -0.778093   -0.03424859]\n",
      " [-0.624707    0.28727895 -0.07767216]] [[-0.97298014  0.23088884  0.05      ]\n",
      " [ 0.907769   -0.41947043  0.05      ]\n",
      " [-0.9864639  -0.16397858  0.05      ]\n",
      " [-0.7780635  -0.62818563  0.05      ]\n",
      " [-0.86873937  0.49526957  0.05      ]]\n",
      "[[-0.01454825  0.85621965  0.01075587]\n",
      " [-0.9661019  -0.12812951  0.07142156]\n",
      " [-0.9275473  -0.32177743  0.08015978]\n",
      " [-0.6468597  -0.7235608   0.07442077]\n",
      " [-0.9258302  -0.32712606  0.08028439]] [[-0.15842567  0.9873709   0.05      ]\n",
      " [-0.958885   -0.2837949   0.05      ]\n",
      " [-0.87212604 -0.4892813   0.05      ]\n",
      " [-0.47364676 -0.8807149   0.05      ]\n",
      " [-0.8689784  -0.49485     0.05      ]]\n",
      "[[-0.91324025 -0.17026807  0.17231615]\n",
      " [ 1.0173324   0.22206074  0.08590955]\n",
      " [-0.87385    -0.4693209   0.19471619]\n",
      " [-0.21000837 -1.1539912   0.18008095]\n",
      " [-0.664073    0.5045193   0.08223398]] [[-0.9962585  -0.08642317  0.05      ]\n",
      " [ 0.89463335  0.446801    0.05      ]\n",
      " [-0.9284208  -0.37153035  0.05      ]\n",
      " [-0.19908197 -0.97998285  0.05      ]\n",
      " [-0.8039206   0.5947366   0.05      ]]\n",
      "[[-3.12117040e-01  9.84381020e-01 -8.26367736e-03]\n",
      " [-6.45134971e-02  1.01566148e+00 -7.28600076e-04]\n",
      " [-5.99144876e-01 -8.28026891e-01 -1.40059292e-02]\n",
      " [-5.76424241e-01 -8.39833617e-01 -1.57835800e-02]\n",
      " [ 9.40649867e-01  4.24672246e-01 -1.01343645e-02]] [[-0.32283333  0.94645584  0.05      ]\n",
      " [-0.0901275   0.99593025  0.05      ]\n",
      " [-0.43926224 -0.8983589   0.05      ]\n",
      " [-0.41656822 -0.90910447  0.05      ]\n",
      " [ 0.9278071   0.3730603   0.05      ]]\n",
      "[[-0.20638438 -1.0428774   0.11974616]\n",
      " [ 0.9037312  -0.4123275   0.11016706]\n",
      " [ 0.24698088 -1.0009651   0.11710469]\n",
      " [-0.47267044  0.9165027   0.13887607]\n",
      " [ 0.52742916 -0.8698108   0.12094728]] [[-0.13537739 -0.9907941   0.05      ]\n",
      " [ 0.8835482  -0.46834025  0.05      ]\n",
      " [ 0.2688355  -0.9631861   0.05      ]\n",
      " [-0.39961308  0.9166839   0.05      ]\n",
      " [ 0.5182744  -0.85521436  0.05      ]]\n",
      "[[ 0.90893185 -0.44035882  0.11396866]\n",
      " [-0.77431256  0.6188814   0.14567682]\n",
      " [ 0.916428   -0.41373914  0.11272165]\n",
      " [-0.63350946  0.7341936   0.14289011]\n",
      " [-1.0409423   0.03004572  0.18232535]] [[ 0.92035544 -0.391083    0.05      ]\n",
      " [-0.7042583   0.70994383  0.05      ]\n",
      " [ 0.9302626  -0.3668944   0.05      ]\n",
      " [-0.5639788   0.8257893   0.05      ]\n",
      " [-0.9904875   0.1376028   0.05      ]]\n",
      "[[-0.178961    1.0113665   0.03345585]\n",
      " [-0.25511533 -0.9806254  -0.01301735]\n",
      " [ 0.9199916   0.341764    0.03217922]\n",
      " [ 0.5862799   0.78667736  0.05148289]\n",
      " [ 0.23721252  0.9669176   0.05389758]] [[-0.16029792  0.98706865  0.05      ]\n",
      " [-0.2798478  -0.9600444   0.05      ]\n",
      " [ 0.9504726   0.31080833  0.05      ]\n",
      " [ 0.6226345   0.78251284  0.05      ]\n",
      " [ 0.26233116  0.9649779   0.05      ]]\n",
      "[[-0.52401     0.96275157  0.08513048]\n",
      " [ 0.11641619 -1.0575603   0.03443419]\n",
      " [ 0.9359027   0.33147573  0.07004264]\n",
      " [ 0.01099441 -1.0593593   0.03097851]\n",
      " [-0.80222577 -0.64312536  0.06008597]] [[-0.4864854   0.8736887   0.05      ]\n",
      " [ 0.11084221 -0.993838    0.05      ]\n",
      " [ 0.9561614   0.29284033  0.05      ]\n",
      " [ 0.01588112 -0.9998739   0.05      ]\n",
      " [-0.7571159  -0.65328056  0.05      ]]\n",
      "[[ 0.21917309  0.9412292   0.15094075]\n",
      " [ 0.5816511   0.75831074  0.1434459 ]\n",
      " [ 0.4908267   0.8223199   0.14752719]\n",
      " [ 0.48031908  0.8287851   0.1479028 ]\n",
      " [ 0.9020999  -0.42447075  0.13334174]] [[ 0.25624928  0.9666107   0.05      ]\n",
      " [ 0.6354137   0.77217185  0.05      ]\n",
      " [ 0.5399439   0.84170103  0.05      ]\n",
      " [ 0.52890116  0.8486834   0.05      ]\n",
      " [ 0.9085483  -0.4177798   0.05      ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90/150, train loss: 0.5678241848945618, \tTEST loss: 0.6004126667976379\n",
      "[[-0.67103934  0.7956205  -0.04024941]\n",
      " [ 0.81269306 -0.5702361   0.09511495]\n",
      " [ 0.47962052  0.85716385  0.01985152]\n",
      " [-0.8835152  -0.45026973  0.052175  ]\n",
      " [ 0.95061517 -0.1282262   0.06455217]] [[-0.69241834  0.7214963   0.05      ]\n",
      " [ 0.82231593 -0.5690312   0.05      ]\n",
      " [ 0.494244    0.8693232   0.05      ]\n",
      " [-0.8636909  -0.5040218   0.05      ]\n",
      " [ 0.9879422  -0.15482305  0.05      ]]\n",
      "[[ 0.20203876 -1.0839151   0.13612288]\n",
      " [-0.9820722  -0.0788595   0.10433131]\n",
      " [-0.62588596  0.7561299   0.03554223]\n",
      " [ 0.8457506   0.4679131   0.0734833 ]\n",
      " [ 0.9401107  -0.34069857  0.11786874]] [[ 0.20834908 -0.9780545   0.05      ]\n",
      " [-0.99864954 -0.05195306  0.05      ]\n",
      " [-0.6513585   0.7587701   0.05      ]\n",
      " [ 0.87859404  0.47756934  0.05      ]\n",
      " [ 0.9472265  -0.32056496  0.05      ]]\n",
      "[[-1.0152123   0.21953957  0.10527744]\n",
      " [ 0.82374203 -0.5235247   0.11665021]\n",
      " [ 0.93505156  0.23718967  0.07684854]\n",
      " [-0.55960643  0.8949049   0.06534521]\n",
      " [ 0.6471634   0.7742695   0.08020737]] [[-0.98048806  0.19657862  0.05      ]\n",
      " [ 0.8338068  -0.5520563   0.05      ]\n",
      " [ 0.9890977   0.14726076  0.05      ]\n",
      " [-0.5313322   0.84716356  0.05      ]\n",
      " [ 0.6991351   0.71498954  0.05      ]]\n",
      "[[-0.3957218  -1.0254405   0.08480001]\n",
      " [ 0.94435644  0.22980186  0.06269071]\n",
      " [-0.6521193   0.7169388   0.05058412]\n",
      " [ 0.57378495 -0.883335    0.10060483]\n",
      " [ 0.7225269   0.6504672   0.06887832]] [[-0.344904   -0.938638    0.05      ]\n",
      " [ 0.9773101   0.21181351  0.05      ]\n",
      " [-0.6646147   0.7471863   0.05      ]\n",
      " [ 0.5706954  -0.82116187  0.05      ]\n",
      " [ 0.7456214   0.66636986  0.05      ]]\n",
      "[[ 0.14057916 -0.98336905  0.07825848]\n",
      " [-0.56099856 -0.84803015  0.09195061]\n",
      " [ 0.7278493  -0.6604455   0.08614518]\n",
      " [ 0.12773575 -0.9854042   0.07804678]\n",
      " [ 0.20749655 -0.9700943   0.07950726]] [[ 0.17234614 -0.98503643  0.05      ]\n",
      " [-0.50416434 -0.8636077   0.05      ]\n",
      " [ 0.7214417  -0.6924752   0.05      ]\n",
      " [ 0.16048379 -0.9870385   0.05      ]\n",
      " [ 0.23403849 -0.97222733  0.05      ]]\n",
      "[[ 0.60904    -0.858956    0.09351134]\n",
      " [ 0.6790589  -0.8008821   0.09423251]\n",
      " [ 0.9668713   0.08241847  0.06373616]\n",
      " [-0.9516991  -0.2708465   0.11711659]\n",
      " [-0.8480658   0.56923085  0.07465798]] [[ 0.6172082  -0.7867999   0.05      ]\n",
      " [ 0.68410766 -0.729381    0.05      ]\n",
      " [ 0.9914096   0.13079366  0.05      ]\n",
      " [-0.9422936  -0.3347876   0.05      ]\n",
      " [-0.8717687   0.4899176   0.05      ]]\n",
      "[[-0.31496423  0.9313713   0.05061531]\n",
      " [-0.9741709   0.1370498   0.06173982]\n",
      " [ 0.27454552 -1.0241528   0.08507856]\n",
      " [ 0.16409853 -1.0511855   0.0796105 ]\n",
      " [-0.9690498  -0.23849674  0.08164025]] [[-0.29783547  0.9546172   0.05      ]\n",
      " [-0.98412645  0.17746872  0.05      ]\n",
      " [ 0.26429182 -0.96444273  0.05      ]\n",
      " [ 0.15973447 -0.98716     0.05      ]\n",
      " [-0.98337805 -0.18156978  0.05      ]]\n",
      "[[-0.9898282   0.34589285  0.06674401]\n",
      " [-0.8479775  -0.62698686  0.09593704]\n",
      " [-0.05690458 -1.0070192   0.07618025]\n",
      " [-0.8502957   0.6449042   0.04831057]\n",
      " [ 0.15231495 -0.97864026  0.08352666]] [[-0.94954556  0.31362915  0.05      ]\n",
      " [-0.78643143 -0.61767757  0.05      ]\n",
      " [ 0.01157974 -0.99993294  0.05      ]\n",
      " [-0.80235827  0.59684277  0.05      ]\n",
      " [ 0.20707758 -0.97832453  0.05      ]]\n",
      "[[-0.98842895  0.0719017   0.08859112]\n",
      " [-0.12845401 -1.0762073   0.08761048]\n",
      " [ 0.27274236  0.9795961   0.08519187]\n",
      " [ 0.26707685 -1.0403054   0.09895886]\n",
      " [ 0.8120722   0.5826249   0.07953139]] [[-0.9954074   0.09572956  0.05      ]\n",
      " [-0.11426342 -0.99345046  0.05      ]\n",
      " [ 0.29217073  0.9563662   0.05      ]\n",
      " [ 0.25838912 -0.9660409   0.05      ]\n",
      " [ 0.85096586  0.525221    0.05      ]]\n",
      "[[-0.6650293   0.7218448   0.03719972]\n",
      " [-0.8128961  -0.5859361   0.09186019]\n",
      " [ 0.42318824  0.88731325  0.07143191]\n",
      " [ 0.03738402 -0.99269557  0.06942331]\n",
      " [-0.77871746 -0.6368525   0.08982995]] [[-0.68949467  0.7242907   0.05      ]\n",
      " [-0.81685203 -0.57684726  0.05      ]\n",
      " [ 0.4453912   0.89533603  0.05      ]\n",
      " [ 0.06895979 -0.99761945  0.05      ]\n",
      " [-0.7776583  -0.6286872   0.05      ]]\n",
      "Step 100/150, train loss: 0.4455399513244629, \tTEST loss: 0.4397723078727722\n",
      "[[-0.9415013   0.27443516  0.07456739]\n",
      " [ 0.839014    0.5036426   0.08318067]\n",
      " [ 0.78406835  0.590501    0.08498567]\n",
      " [ 0.9442046   0.20810196  0.08307554]\n",
      " [ 0.61816937  0.7658977   0.0900964 ]] [[-0.9603046   0.27895337  0.05      ]\n",
      " [ 0.87784714  0.47894096  0.05      ]\n",
      " [ 0.81860775  0.57435304  0.05      ]\n",
      " [ 0.9863936   0.1644012   0.05      ]\n",
      " [ 0.63806325  0.76998395  0.05      ]]\n",
      "[[-0.9002637  -0.52901536  0.09833495]\n",
      " [ 0.8028694  -0.6050622   0.08620789]\n",
      " [-0.96333784 -0.37202615  0.10206512]\n",
      " [ 0.942892    0.30122772  0.0645446 ]\n",
      " [-0.8730116   0.5237223   0.06293934]] [[-0.8501047  -0.5266137   0.05      ]\n",
      " [ 0.81056    -0.5856556   0.05      ]\n",
      " [-0.9268182  -0.3755103   0.05      ]\n",
      " [ 0.95934653  0.28223073  0.05      ]\n",
      " [-0.87378514  0.48631218  0.05      ]]\n",
      "[[ 0.94524884 -0.28490773  0.08279142]\n",
      " [-0.79443794  0.52092314  0.04912974]\n",
      " [ 0.58194196 -0.82924414  0.09298363]\n",
      " [-0.9036837   0.2959947   0.06368124]\n",
      " [-0.916267    0.25611842  0.06658871]] [[ 0.94671595 -0.3220697   0.05      ]\n",
      " [-0.82847446  0.5600268   0.05      ]\n",
      " [ 0.56763434 -0.8232808   0.05      ]\n",
      " [-0.9413641   0.33739233  0.05      ]\n",
      " [-0.9545252   0.2981303   0.05      ]]\n",
      "[[ 0.92004025 -0.3221377   0.07356863]\n",
      " [-0.45905232 -0.9669984   0.05165932]\n",
      " [ 0.41780126 -0.93226576  0.06642416]\n",
      " [ 0.8902627  -0.40998092  0.07577991]\n",
      " [ 0.963915   -0.02361114  0.06556287]] [[ 0.9499015  -0.3125494   0.05      ]\n",
      " [-0.33365497 -0.94269526  0.05      ]\n",
      " [ 0.4805033  -0.87699294  0.05      ]\n",
      " [ 0.9191582  -0.39388847  0.05      ]\n",
      " [ 0.9995659  -0.02946138  0.05      ]]\n",
      "[[ 0.9279569  -0.262646    0.08742007]\n",
      " [ 0.51121724  0.8464409   0.06399433]\n",
      " [-0.5941987  -0.8359824   0.05586963]\n",
      " [ 0.9433082   0.2581319   0.06680278]\n",
      " [-0.45833918 -0.9042804   0.05292743]] [[ 0.9397116  -0.3419681   0.05      ]\n",
      " [ 0.54468626  0.8386399   0.05      ]\n",
      " [-0.5220677  -0.852904    0.05      ]\n",
      " [ 0.9837547   0.17951784  0.05      ]\n",
      " [-0.38573027 -0.9226116   0.05      ]]\n",
      "[[-0.48430917  0.9468178   0.15162794]\n",
      " [ 0.45219132  0.9459992   0.15356119]\n",
      " [ 0.8584448  -0.6056747   0.15657963]\n",
      " [-0.77469647 -0.53573114  0.19374053]\n",
      " [ 0.19023234 -0.96369416  0.15627289]] [[-0.5492495   0.83565843  0.05      ]\n",
      " [ 0.36967656  0.9291605   0.05      ]\n",
      " [ 0.777377   -0.629035    0.05      ]\n",
      " [-0.77431893 -0.6327956   0.05      ]\n",
      " [ 0.15713514 -0.9875771   0.05      ]]\n",
      "[[ 0.4650209   0.68234813 -0.0347732 ]\n",
      " [-0.88756335 -0.63269085 -0.0746969 ]\n",
      " [-0.8773559  -0.65247846 -0.07509742]\n",
      " [-0.92309046 -0.5520768  -0.07374862]\n",
      " [ 0.37566155  0.727932   -0.03568339]] [[ 0.572327    0.8200255   0.05      ]\n",
      " [-0.88947034 -0.45699286  0.05      ]\n",
      " [-0.87795246 -0.47874779  0.05      ]\n",
      " [-0.9294454  -0.36895975  0.05      ]\n",
      " [ 0.4757403   0.8795858   0.05      ]]\n",
      "[[-0.68193185 -0.5208984   0.3358825 ]\n",
      " [-0.85286283 -0.29027516  0.32300532]\n",
      " [ 0.9293807   0.01191308  0.21824127]\n",
      " [ 0.92617565  0.18826456  0.19373932]\n",
      " [-0.94487214 -0.06280954  0.2994066 ]] [[-0.68916035 -0.72460884  0.05      ]\n",
      " [-0.8668462  -0.49857566  0.05      ]\n",
      " [ 0.9879341  -0.15487494  0.05      ]\n",
      " [ 0.9995717   0.02926464  0.05      ]\n",
      " [-0.9623815  -0.27170184  0.05      ]]\n",
      "[[ 0.57820594 -1.1080116   0.03310503]\n",
      " [ 0.9403109  -0.7946095   0.03566455]\n",
      " [ 1.1119763  -0.22443278  0.02303017]\n",
      " [ 1.1065099  -0.30477133  0.02402727]\n",
      " [ 0.86451566 -0.8948431   0.03697309]] [[ 0.52334183 -0.85212284  0.05      ]\n",
      " [ 0.84510493 -0.53460044  0.05      ]\n",
      " [ 0.9999302  -0.01181693  0.05      ]\n",
      " [ 0.99636763 -0.08515622  0.05      ]\n",
      " [ 0.7765143  -0.6300996   0.05      ]]\n",
      "[[ 0.6473189   0.9175471   0.03776475]\n",
      " [-0.49682602 -0.84448665 -0.08143722]\n",
      " [-0.34363058  1.2658505   0.0543821 ]\n",
      " [-0.10077489 -0.8887655  -0.08175899]\n",
      " [ 0.3551723  -0.71806824 -0.04876094]] [[ 0.85844296  0.512909    0.05      ]\n",
      " [-0.3972054  -0.91772974  0.05      ]\n",
      " [-0.12807092  0.991765    0.05      ]\n",
      " [-0.00532289 -0.9999858   0.05      ]\n",
      " [ 0.42877305 -0.9034122   0.05      ]]\n",
      "Step 110/150, train loss: 2.0705881118774414, \tTEST loss: 1.3690327405929565\n",
      "[[-0.86153    -0.46383643  0.05719816]\n",
      " [-0.4133254   0.83864766  0.12041546]\n",
      " [ 0.6829102   0.6631132   0.21249512]\n",
      " [-0.6184137  -0.9077655   0.06145002]\n",
      " [-0.7801461   0.42646623  0.0628036 ]] [[-0.97771496 -0.20993674  0.05      ]\n",
      " [-0.3319817   0.9432858   0.05      ]\n",
      " [ 0.87946075  0.47597137  0.05      ]\n",
      " [-0.7243241  -0.6894596   0.05      ]\n",
      " [-0.78566545  0.61865157  0.05      ]]\n",
      "[[-0.45017323  0.8648098   0.1186839 ]\n",
      " [-0.73992306  0.62497675  0.0894347 ]\n",
      " [ 0.751072    0.01960547  0.21636865]\n",
      " [-0.91744494 -0.14078963  0.09806546]\n",
      " [-0.6120536  -0.6434363   0.10282619]] [[-0.47642553  0.8792148   0.05      ]\n",
      " [-0.8035243   0.5952719   0.05      ]\n",
      " [ 0.9829114  -0.18407924  0.05      ]\n",
      " [-0.9597387  -0.2808945   0.05      ]\n",
      " [-0.53742343 -0.84331256  0.05      ]]\n",
      "[[-0.5933269   0.69893587  0.07733575]\n",
      " [ 0.8470893   0.42472085  0.16154815]\n",
      " [ 0.2596244  -0.9353103   0.16483265]\n",
      " [-0.88683873 -0.0489706   0.09175432]\n",
      " [ 0.7893234  -0.46935278  0.19596551]] [[-0.7078967   0.70631593  0.05      ]\n",
      " [ 0.9391649   0.34346667  0.05      ]\n",
      " [ 0.2708168  -0.9626309   0.05      ]\n",
      " [-0.9972921  -0.07354203  0.05      ]\n",
      " [ 0.83354515 -0.5524513   0.05      ]]\n",
      "[[1.0218468  0.16886961 0.07725413]\n",
      " [0.9366061  0.46996298 0.06699821]\n",
      " [0.84426767 0.6327592  0.06642471]\n",
      " [0.82599497 0.65762275 0.06663967]\n",
      " [0.333913   0.9782736  0.07001176]] [[0.991708   0.12851194 0.05      ]\n",
      " [0.903938   0.4276636  0.05      ]\n",
      " [0.8015016  0.5979927  0.05      ]\n",
      " [0.7810363  0.6244856  0.05      ]\n",
      " [0.23922686 0.9709637  0.05      ]]\n",
      "[[-0.9288777  -0.74797827  0.13761827]\n",
      " [ 0.94492656 -0.6417981   0.12183471]\n",
      " [-0.6873141   0.715125    0.04126017]\n",
      " [ 0.88840586 -0.77006114  0.13245845]\n",
      " [ 0.7759113  -0.94730604  0.14418861]] [[-0.8773431  -0.4798636   0.05      ]\n",
      " [ 0.8555767  -0.5176761   0.05      ]\n",
      " [-0.6097755   0.79257417  0.05      ]\n",
      " [ 0.79079854 -0.61207646  0.05      ]\n",
      " [ 0.67314416 -0.7395113   0.05      ]]\n",
      "[[ 0.25438446  0.942315    0.03723268]\n",
      " [-0.76016283  0.69185853  0.02747933]\n",
      " [-0.02467828  0.97308654  0.0343863 ]\n",
      " [-0.84231454  0.60526747  0.03284854]\n",
      " [ 0.5864959   0.7851437   0.03511478]] [[ 0.36570382  0.9307313   0.05      ]\n",
      " [-0.66833466  0.7438607   0.05      ]\n",
      " [ 0.07646251  0.99707246  0.05      ]\n",
      " [-0.7533432   0.6576275   0.05      ]\n",
      " [ 0.712466    0.7017066   0.05      ]]\n",
      "[[ 0.8342748   0.4962162   0.02466989]\n",
      " [-0.7116078  -0.54649943  0.0913064 ]\n",
      " [ 0.7569451  -0.39401066  0.07183226]\n",
      " [ 0.5182462   0.883446    0.040021  ]\n",
      " [-0.800798   -0.42634648  0.09516823]] [[ 0.9493915   0.31409517  0.05      ]\n",
      " [-0.72416735 -0.68962425  0.05      ]\n",
      " [ 0.7870183  -0.61692965  0.05      ]\n",
      " [ 0.5861324   0.8102153   0.05      ]\n",
      " [-0.82544625 -0.5644807   0.05      ]]\n",
      "[[ 0.6012112  -0.8135694   0.11127253]\n",
      " [-0.9225342  -0.13296615  0.1263697 ]\n",
      " [-0.63508415  0.79269695  0.10479573]\n",
      " [-0.6786613  -0.6497697   0.11302739]\n",
      " [-0.9006526   0.37228578  0.11345095]] [[ 0.6020499  -0.79845846  0.05      ]\n",
      " [-0.95947504 -0.28179368  0.05      ]\n",
      " [-0.7382719   0.6745032   0.05      ]\n",
      " [-0.6599344  -0.7513232   0.05      ]\n",
      " [-0.97705257  0.21299817  0.05      ]]\n",
      "[[ 1.0170877  -0.02477008  0.12149047]\n",
      " [-0.04353995 -1.1826338   0.11037093]\n",
      " [-0.6009148   0.7056929   0.09316425]\n",
      " [ 0.35251233  0.89677477  0.14869043]\n",
      " [ 0.33966726 -1.1446234   0.12863553]] [[ 0.99986464  0.01645472  0.05      ]\n",
      " [-0.07374762 -0.99727696  0.05      ]\n",
      " [-0.6859664   0.72763324  0.05      ]\n",
      " [ 0.26415145  0.96448123  0.05      ]\n",
      " [ 0.28393596 -0.95884323  0.05      ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16497132 -1.0579126   0.07622175]\n",
      " [-0.9609563  -0.41707873  0.03573532]\n",
      " [-0.83392435 -0.6991509   0.04377664]\n",
      " [ 0.89572614 -0.345271    0.09346955]\n",
      " [ 0.741348    0.6758537   0.06876264]] [[ 0.13956566 -0.9902128   0.05      ]\n",
      " [-0.9584806  -0.28515768  0.05      ]\n",
      " [-0.8273832  -0.56163776  0.05      ]\n",
      " [ 0.89357316 -0.44891757  0.05      ]\n",
      " [ 0.8111266   0.5848707   0.05      ]]\n",
      "Step 120/150, train loss: 0.5927363634109497, \tTEST loss: 0.4928937554359436\n",
      "[[ 0.8344693   0.43960243  0.04748266]\n",
      " [ 0.02085377  0.9528089   0.04249745]\n",
      " [-0.7963192  -0.6915502   0.07074662]\n",
      " [-1.0012431   0.11363963  0.03195672]\n",
      " [ 0.3659318   0.88880444  0.05218482]] [[ 0.9338786   0.35759023  0.05      ]\n",
      " [ 0.04540955  0.9989685   0.05      ]\n",
      " [-0.7523932  -0.6587143   0.05      ]\n",
      " [-0.9879813   0.1545734   0.05      ]\n",
      " [ 0.4167686   0.9090126   0.05      ]]\n",
      "[[ 0.64801455  0.73407423  0.08327799]\n",
      " [ 0.31887564  0.92267275  0.09947403]\n",
      " [-0.9912431  -0.05634962  0.14528495]\n",
      " [ 0.6240901   0.7552583   0.08489536]\n",
      " [-0.86831295 -0.5453795   0.16282372]] [[ 0.6823635   0.73101306  0.05      ]\n",
      " [ 0.31140995  0.95027566  0.05      ]\n",
      " [-0.99853545 -0.05410074  0.05      ]\n",
      " [ 0.6548468   0.7557617   0.05      ]\n",
      " [-0.846321   -0.53267324  0.05      ]]\n",
      "[[ 0.69249046  0.70741427  0.04966354]\n",
      " [ 0.84769297  0.4795947   0.04578256]\n",
      " [ 0.6777008  -0.67058474  0.09852139]\n",
      " [ 0.5576447  -0.78235763  0.10007507]\n",
      " [-0.92738634  0.3984675   0.03039511]] [[ 0.72779506  0.6857947   0.05      ]\n",
      " [ 0.90407234  0.42737946  0.05      ]\n",
      " [ 0.70077974 -0.7133777   0.05      ]\n",
      " [ 0.58150005 -0.81354636  0.05      ]\n",
      " [-0.92936444  0.36916357  0.05      ]]\n",
      "[[-1.000267   -0.13715443  0.10455405]\n",
      " [-0.26485583 -0.98036134  0.09712222]\n",
      " [ 0.62757343 -0.78031343  0.11300724]\n",
      " [-0.99218917  0.16031598  0.08975625]\n",
      " [ 0.29897776  0.92766976  0.10075562]] [[-0.9883337  -0.15230381  0.05      ]\n",
      " [-0.21101788 -0.9774822   0.05      ]\n",
      " [ 0.6232604  -0.78201437  0.05      ]\n",
      " [-0.99020416  0.13962702  0.05      ]\n",
      " [ 0.25840846  0.9660357   0.05      ]]\n",
      "[[-0.96291    -0.28852314  0.09022871]\n",
      " [-0.989138   -0.08806586  0.08038546]\n",
      " [ 0.6538657   0.7280622   0.08083123]\n",
      " [-0.611124   -0.8360518   0.09396448]\n",
      " [-0.33177772 -0.9732957   0.09124691]] [[-0.9679182  -0.25126556  0.05      ]\n",
      " [-0.99868107 -0.05134359  0.05      ]\n",
      " [ 0.6632741   0.74837655  0.05      ]\n",
      " [-0.5862454  -0.8101335   0.05      ]\n",
      " [-0.3034755  -0.95283926  0.05      ]]\n",
      "[[ 0.7874856   0.58305836  0.05372212]\n",
      " [-0.74966043  0.64804476  0.00654329]\n",
      " [-0.9391033   0.32603806  0.01793764]\n",
      " [ 0.9126266  -0.29502213  0.07859312]\n",
      " [ 0.07821033 -0.9928831   0.07539434]] [[ 0.82701665  0.5621775   0.05      ]\n",
      " [-0.75154704  0.6596795   0.05      ]\n",
      " [-0.9416021   0.33672756  0.05      ]\n",
      " [ 0.93693215 -0.34951124  0.05      ]\n",
      " [ 0.1034299  -0.9946368   0.05      ]]\n",
      "[[-0.63630617  0.780837    0.05489559]\n",
      " [ 0.62913895  0.7713444   0.08260076]\n",
      " [ 0.0338658   0.98647165  0.08244852]\n",
      " [-0.00430865  0.9872362   0.08119104]\n",
      " [-1.0019815  -0.05998041  0.09384057]] [[-0.6385081   0.7696151   0.05      ]\n",
      " [ 0.6314398   0.7754249   0.05      ]\n",
      " [ 0.01215365  0.99992615  0.05      ]\n",
      " [-0.02598659  0.9996623   0.05      ]\n",
      " [-0.99852145 -0.05435943  0.05      ]]\n",
      "[[-0.54634535  0.8176926   0.04551236]\n",
      " [ 0.9374193   0.29464802  0.0703584 ]\n",
      " [ 0.78217745  0.60535693  0.07175931]\n",
      " [-0.9403848  -0.35041028  0.09788275]\n",
      " [ 0.8830704  -0.4656908   0.1046328 ]] [[-0.5491133   0.8357479   0.05      ]\n",
      " [ 0.96497804  0.26233068  0.05      ]\n",
      " [ 0.8020213   0.59729534  0.05      ]\n",
      " [-0.94705266 -0.32107833  0.05      ]\n",
      " [ 0.8740214  -0.48588738  0.05      ]]\n",
      "[[ 0.9534263   0.14599884  0.06325173]\n",
      " [ 0.6593982   0.7243744   0.07164736]\n",
      " [ 0.32557705 -0.96138424  0.08708411]\n",
      " [-0.9643383   0.3949524   0.05651591]\n",
      " [-0.87167835  0.5721108   0.04830815]] [[ 0.9907383   0.13578534  0.05      ]\n",
      " [ 0.67638654  0.7365469   0.05      ]\n",
      " [ 0.3679067  -0.92986274  0.05      ]\n",
      " [-0.9295865   0.36860397  0.05      ]\n",
      " [-0.8383129   0.5451894   0.05      ]]\n",
      "[[ 0.07015572  0.94640696  0.07967613]\n",
      " [-0.6441388  -0.8026959   0.10740119]\n",
      " [ 0.9761258   0.10071662  0.06758497]\n",
      " [ 0.5516174  -0.8600894   0.10589799]\n",
      " [ 0.98100173  0.0084841   0.07058308]] [[ 0.04602863  0.9989401   0.05      ]\n",
      " [-0.6528804  -0.7574611   0.05      ]\n",
      " [ 0.99789363  0.06487104  0.05      ]\n",
      " [ 0.5305515  -0.84765273  0.05      ]\n",
      " [ 0.999587   -0.02873751  0.05      ]]\n",
      "Step 130/150, train loss: 0.42623186111450195, \tTEST loss: 0.4542730450630188\n",
      "[[ 0.00677642 -0.99661195  0.06829952]\n",
      " [ 0.23027939  0.97802645  0.06555787]\n",
      " [-0.94704974 -0.28838927  0.0797867 ]\n",
      " [-0.56451815 -0.8352081   0.07321229]\n",
      " [-0.9144239   0.40526554  0.04429433]] [[ 0.03164133 -0.99949926  0.05      ]\n",
      " [ 0.22193284  0.97506195  0.05      ]\n",
      " [-0.9566528  -0.2912307   0.05      ]\n",
      " [-0.53938824 -0.84205717  0.05      ]\n",
      " [-0.92201966  0.38714302  0.05      ]]\n",
      "[[-0.44473934  0.9016982   0.04906038]\n",
      " [-0.38759157 -0.9665131   0.09800898]\n",
      " [-0.9743448   0.24901244  0.06892581]\n",
      " [-0.8843537  -0.52425486  0.10863902]\n",
      " [ 0.5195151  -0.86485654  0.10505082]] [[-0.43298993  0.9013988   0.05      ]\n",
      " [-0.33264667 -0.9430515   0.05      ]\n",
      " [-0.966505    0.25664774  0.05      ]\n",
      " [-0.8655211  -0.5008725   0.05      ]\n",
      " [ 0.5480647  -0.836436    0.05      ]]\n",
      "[[-0.9574312  -0.10758193  0.07702433]\n",
      " [ 0.9712686   0.01770025  0.06226771]\n",
      " [ 0.6332262   0.7607777   0.06501853]\n",
      " [ 0.82248217  0.55567956  0.05744401]\n",
      " [ 0.6141865   0.77520454  0.06566543]] [[-0.9973523  -0.07272165  0.05      ]\n",
      " [ 0.99663174 -0.08200705  0.05      ]\n",
      " [ 0.66155     0.74990106  0.05      ]\n",
      " [ 0.86135375  0.5080056   0.05      ]\n",
      " [ 0.64133984  0.7672569   0.05      ]]\n",
      "[[ 0.45743394 -0.8938551   0.08959135]\n",
      " [ 0.6931446   0.7156989   0.08344078]\n",
      " [ 0.01753238 -1.0112273   0.080224  ]\n",
      " [-0.9060395   0.42341945  0.06891728]\n",
      " [-1.003858   -0.17015652  0.09596228]] [[ 0.45763    -0.88914275  0.05      ]\n",
      " [ 0.69768536  0.7164043   0.05      ]\n",
      " [ 0.04629203 -0.99892795  0.05      ]\n",
      " [-0.8885718   0.45873755  0.05      ]\n",
      " [-0.9924519  -0.12263432  0.05      ]]\n",
      "[[-0.67783046 -0.73624945  0.07929095]\n",
      " [-0.5021709  -0.85453784  0.07284212]\n",
      " [-0.1865417   1.014328    0.06117766]\n",
      " [ 0.21810748 -0.94484013  0.07355649]\n",
      " [-0.6572425  -0.7533311   0.07844153]] [[-0.61037326 -0.79211396  0.05      ]\n",
      " [-0.42558753 -0.90491724  0.05      ]\n",
      " [-0.20732708  0.97827166  0.05      ]\n",
      " [ 0.27086896 -0.9626162   0.05      ]\n",
      " [-0.5882862  -0.8086528   0.05      ]]\n",
      "[[ 0.9745936   0.1789089   0.06715806]\n",
      " [ 0.87611735  0.46352082  0.059333  ]\n",
      " [-0.74986035  0.5708728   0.0173222 ]\n",
      " [ 0.96252817 -0.2825586   0.09598455]\n",
      " [ 0.2597787  -0.981736    0.11816484]] [[ 0.99197483  0.12643571  0.05      ]\n",
      " [ 0.89989954  0.43609726  0.05      ]\n",
      " [-0.79644734  0.60470784  0.05      ]\n",
      " [ 0.9397667  -0.3418165   0.05      ]\n",
      " [ 0.17199345 -0.98509806  0.05      ]]\n",
      "[[ 0.9215696   0.21079     0.07155548]\n",
      " [-0.74728954 -0.6907591   0.07488383]\n",
      " [ 0.9569136  -0.11075631  0.07743095]\n",
      " [ 0.49162957  0.79473585  0.08524779]\n",
      " [-0.9875527  -0.17897004  0.06851768]] [[ 0.9727751   0.23175128  0.05      ]\n",
      " [-0.7429448  -0.6693527   0.05      ]\n",
      " [ 0.9953065  -0.09677283  0.05      ]\n",
      " [ 0.5261325   0.8504026   0.05      ]\n",
      " [-0.9875998  -0.15699239  0.05      ]]\n",
      "[[ 0.60224456  0.7976539   0.09833135]\n",
      " [ 0.9933392  -0.24257953  0.09897452]\n",
      " [-0.25014478  0.9292448   0.08048882]\n",
      " [-0.7299654   0.63082397  0.0594896 ]\n",
      " [ 0.29109427  0.93454784  0.10058683]] [[ 0.569801    0.82178277  0.05      ]\n",
      " [ 0.9647479  -0.2631757   0.05      ]\n",
      " [-0.29367578  0.9559051   0.05      ]\n",
      " [-0.760305    0.6495663   0.05      ]\n",
      " [ 0.24569273  0.9693478   0.05      ]]\n",
      "[[-0.9362177  -0.51305366  0.10346642]\n",
      " [ 0.85159427 -0.4585494   0.08213834]\n",
      " [-1.1044616  -0.15084457  0.10248161]\n",
      " [ 0.26124635  1.0284694   0.05474686]\n",
      " [-0.6212975  -0.8009344   0.08812983]] [[-0.7799171  -0.62588274  0.05      ]\n",
      " [ 0.8781408  -0.4784023   0.05      ]\n",
      " [-0.959735   -0.28090698  0.05      ]\n",
      " [ 0.3000412   0.95392627  0.05      ]\n",
      " [-0.46043295 -0.8876945   0.05      ]]\n",
      "[[-0.46347493 -0.89941454  0.0778477 ]\n",
      " [-0.4538841  -0.9057542   0.07793815]\n",
      " [-0.6063491  -0.7821693   0.0758279 ]\n",
      " [ 0.91010696 -0.59012026  0.08499157]\n",
      " [ 0.69270957  0.6941126   0.02460053]] [[-0.63464314 -0.77280533  0.05      ]\n",
      " [-0.625457   -0.78025866  0.05      ]\n",
      " [-0.76957977 -0.6385507   0.05      ]\n",
      " [ 0.7853793  -0.61901486  0.05      ]\n",
      " [ 0.7051191   0.7090889   0.05      ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 140/150, train loss: 0.8137954473495483, \tTEST loss: 0.8151288032531738\n",
      "[[-0.9761071  -0.03672208  0.07924249]\n",
      " [ 0.60998785 -0.60583425  0.11149433]\n",
      " [ 0.6775722   0.60656846  0.08730429]\n",
      " [ 0.5367351  -0.67761177  0.11118323]\n",
      " [-0.3495401   0.9186198   0.06598531]] [[-0.99860734 -0.05275736  0.05      ]\n",
      " [ 0.7216464  -0.6922618   0.05      ]\n",
      " [ 0.81027645  0.5860479   0.05      ]\n",
      " [ 0.6431724  -0.7657214   0.05      ]\n",
      " [-0.30937696  0.9509395   0.05      ]]\n",
      "[[ 0.43787393 -0.9861184   0.05454161]\n",
      " [-0.98474324 -0.23475279  0.03909736]\n",
      " [ 0.7880348   0.5433284   0.07344401]\n",
      " [ 0.20884913  0.94169164  0.08645986]\n",
      " [ 0.7751676   0.5624928   0.07436723]] [[ 0.49168152 -0.8707751   0.05      ]\n",
      " [-0.98613745 -0.16593051  0.05      ]\n",
      " [ 0.7995906   0.6005455   0.05      ]\n",
      " [ 0.18624604  0.9825031   0.05      ]\n",
      " [ 0.7852328   0.6192007   0.05      ]]\n",
      "[[ 0.35726646  1.0335828   0.07651279]\n",
      " [-0.96049905 -0.23315854  0.14617369]\n",
      " [ 0.64156044  0.868803    0.06629754]\n",
      " [-0.0366688   1.1117953   0.07951546]\n",
      " [ 0.11213233 -0.9718689   0.12222628]] [[ 0.31731865  0.94831896  0.05      ]\n",
      " [-0.9408461  -0.33883426  0.05      ]\n",
      " [ 0.6005163   0.7996125   0.05      ]\n",
      " [-0.06238114  0.9980524   0.05      ]\n",
      " [ 0.12234287 -0.9924879   0.05      ]]\n",
      "[[-0.23433575  0.8041872  -0.04847509]\n",
      " [ 0.64722425  0.6561712  -0.00208388]\n",
      " [-0.29951483 -1.0929291  -0.00423804]\n",
      " [-0.6999143  -0.9046359  -0.01144749]\n",
      " [-0.8914098  -0.6669538  -0.01907656]] [[-0.15548003  0.98783904  0.05      ]\n",
      " [ 0.7108007   0.70339346  0.05      ]\n",
      " [-0.3073507  -0.9515963   0.05      ]\n",
      " [-0.7181305  -0.6959084   0.05      ]\n",
      " [-0.91140467 -0.41151127  0.05      ]]\n",
      "[[-0.8989806   0.42925537  0.11070295]\n",
      " [-0.62780887 -0.59837884  0.20365752]\n",
      " [-0.11000919  0.9614774   0.07620149]\n",
      " [ 0.6248778   0.7732744   0.08515155]\n",
      " [ 0.9361932  -0.03553648  0.13451372]] [[-0.9165169   0.39999592  0.05      ]\n",
      " [-0.63139397 -0.7754622   0.05      ]\n",
      " [-0.10857657  0.9940881   0.05      ]\n",
      " [ 0.6589302   0.7522041   0.05      ]\n",
      " [ 0.9823122  -0.18725036  0.05      ]]\n",
      "[[ 0.934296   -0.34231344  0.13405387]\n",
      " [-0.01594108  1.0133182   0.09322307]\n",
      " [-0.9025677   0.4683712   0.0949988 ]\n",
      " [ 0.32028165 -0.91012526  0.16113196]\n",
      " [ 0.85177106  0.57848245  0.09300898]] [[ 0.927888   -0.37285912  0.05      ]\n",
      " [-0.04718801  0.99888605  0.05      ]\n",
      " [-0.90218496  0.43134934  0.05      ]\n",
      " [ 0.3364119  -0.94171494  0.05      ]\n",
      " [ 0.83595765  0.5487939   0.05      ]]\n",
      "[[-1.015721    0.107545    0.02251275]\n",
      " [ 0.8982919  -0.38869214  0.04348358]\n",
      " [-0.9941149  -0.22981004  0.039494  ]\n",
      " [ 0.7121586  -0.69059986  0.05161016]\n",
      " [ 0.20024608  0.9865363   0.03646633]] [[-0.992301    0.12384954  0.05      ]\n",
      " [ 0.9175556  -0.39760756  0.05      ]\n",
      " [-0.9771768  -0.21242784  0.05      ]\n",
      " [ 0.73081446 -0.6825762   0.05      ]\n",
      " [ 0.21903461  0.97571707  0.05      ]]\n",
      "[[-0.06384172 -0.9994602   0.12628092]\n",
      " [-0.9959499  -0.11114618  0.12359264]\n",
      " [ 0.4078402   0.9042651   0.10581829]\n",
      " [ 0.8274892  -0.5362504   0.12329485]\n",
      " [ 0.9583477  -0.05130696  0.10023186]] [[-0.0277243  -0.9996156   0.05      ]\n",
      " [-0.9949764  -0.1001098   0.05      ]\n",
      " [ 0.43894938  0.89851177  0.05      ]\n",
      " [ 0.84974146 -0.5271996   0.05      ]\n",
      " [ 0.99783224 -0.06580941  0.05      ]]\n",
      "[[ 0.9146945   0.38385233  0.03553447]\n",
      " [-0.3928327  -0.89673036  0.07374407]\n",
      " [-0.3344577   0.9704911   0.04040282]\n",
      " [ 0.53158754  0.8517857   0.05143081]\n",
      " [ 0.32872784  0.9513126   0.05500361]] [[ 0.9327421   0.36054423  0.05      ]\n",
      " [-0.38712248 -0.9220283   0.05      ]\n",
      " [-0.3180055   0.9480889   0.05      ]\n",
      " [ 0.5415324   0.8406799   0.05      ]\n",
      " [ 0.33639574  0.9417207   0.05      ]]\n",
      "[[-0.02549543 -1.0143884   0.07691653]\n",
      " [ 0.0075678  -1.015815    0.07584722]\n",
      " [-0.71740395 -0.6897161   0.12058949]\n",
      " [ 0.01435326 -1.0159838   0.07563815]\n",
      " [-0.9519345  -0.17627694  0.12985933]] [[-0.05866088 -0.99827796  0.05      ]\n",
      " [-0.02613333 -0.99965847  0.05      ]\n",
      " [-0.762439   -0.6470601   0.05      ]\n",
      " [-0.01946822 -0.99981046  0.05      ]\n",
      " [-0.993953   -0.10980633  0.05      ]]\n",
      "Step 150/150, train loss: 0.41656315326690674, \tTEST loss: 0.49385738372802734\n",
      "Fin. train loss: 0.41656315326690674, \tTEST loss: 0.49385738372802734\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Training step that optimizes the weights \n",
    "    provided some batch_size X and Y examples from the dataset. \n",
    "    \"\"\"\n",
    "    X, Y = generate_x_y_data(isTrain=True, batch_size=batch_size)\n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(len(enc_inp))}\n",
    "    feed_dict.update({expected_sparse_output[t]: Y[t] for t in range(len(expected_sparse_output))})\n",
    "    _, loss_t, __y, __Y = sess.run([train_op, loss, _y, _Y], feed_dict)\n",
    "    print (__y, __Y)\n",
    "    #print(expected_sparse_output)\n",
    "    \n",
    "    return loss_t\n",
    "\n",
    "def test_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Test step, does NOT optimizes. Weights are frozen by not\n",
    "    doing sess.run on the train_op. \n",
    "    \"\"\"\n",
    "    X, Y = generate_x_y_data(isTrain=False, batch_size=batch_size)\n",
    "    feed_dict = {enc_inp[t]: X[t] for t in range(len(enc_inp))}\n",
    "    feed_dict.update({expected_sparse_output[t]: Y[t] for t in range(len(expected_sparse_output))})\n",
    "    loss_t = sess.run([loss], feed_dict)\n",
    "    return loss_t[0]\n",
    "\n",
    "\n",
    "# Training\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for t in range(nb_iters+1):\n",
    "    train_loss = train_batch(batch_size)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if t % 10 == 0: \n",
    "        # Tester\n",
    "        test_loss = test_batch(batch_size)\n",
    "        test_losses.append(test_loss)\n",
    "        print(\"Step {}/{}, train loss: {}, \\tTEST loss: {}\".format(t, nb_iters, train_loss, test_loss))\n",
    "\n",
    "print(\"Fin. train loss: {}, \\tTEST loss: {}\".format(train_loss, test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (_y)\n",
    "print (_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot loss over time:\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    np.array(range(0, len(test_losses)))/float(len(test_losses)-1)*(len(train_losses)-1), \n",
    "    np.log(test_losses), \n",
    "    label=\"Test loss\"\n",
    ")\n",
    "plt.plot(\n",
    "    np.log(train_losses), \n",
    "    label=\"Train loss\"\n",
    ")\n",
    "plt.title(\"Training errors over time (on a logarithmic scale)\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('log(Loss)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "nb_predictions = 1\n",
    "#print(\"Let's visualize {} predictions with our signals:\".format(nb_predictions))\n",
    "\n",
    "X, Y = generate_x_y_data(isTrain=False, batch_size=nb_predictions)\n",
    "feed_dict = {enc_inp[t]: X[t] for t in range(seq_length)}\n",
    "outputs = np.array(sess.run([reshaped_outputs], feed_dict)[0])\n",
    "\n",
    "for j in range(nb_predictions): \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    \n",
    "    #print (X)\n",
    "    \n",
    "    for k in range(output_dim):\n",
    "        past = X[:,j,k]   # ROB LOOK Here: j is batch number, k is feature (first or second)\n",
    "        expected = Y[:,j,k]\n",
    "        pred = outputs[:,j,k]\n",
    "        \n",
    "        label1 = \"Seen (past) values\" if k==0 else \"_nolegend_\"\n",
    "        label2 = \"True future values\" if k==0 else \"_nolegend_\"\n",
    "        label3 = \"Predictions\" if k==0 else \"_nolegend_\"\n",
    "        plt.plot(range(len(past)), past, \"o--b\", label=label1)\n",
    "        plt.plot(range(len(past), len(expected)+len(past)), expected, \"x--b\", label=label2)\n",
    "        plt.plot(range(len(past), len(pred)+len(past)), pred, \"o--y\", label=label3)\n",
    "        #print (past)\n",
    "        \n",
    "    plt.legend(loc='best')\n",
    "    plt.title(\"Predictions v.s. true values\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Reminder: the signal can contain many dimensions at once.\")\n",
    "print(\"In that case, signals have the same color.\")\n",
    "print(\"In reality, we could imagine multiple stock market symbols evolving,\")\n",
    "print(\"tied in time together and seen at once by the neural network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n",
    "\n",
    "Guillaume Chevalier\n",
    "- https://ca.linkedin.com/in/chevalierg\n",
    "- https://twitter.com/guillaume_che\n",
    "- https://github.com/guillaume-chevalier/\n",
    "\n",
    "## License\n",
    "\n",
    "This project is free to use according to the [MIT License](https://github.com/guillaume-chevalier/seq2seq-signal-prediction/blob/master/LICENSE) as long as you cite me and the License (read the License for more details). You can cite me by pointing to the following link: \n",
    "- https://github.com/guillaume-chevalier/seq2seq-signal-prediction\n",
    "\n",
    "## Converting notebook to a readme file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert this notebook to a README for the GitHub project's title page:\n",
    "!jupyter nbconvert --to markdown seq2seq.ipynb\n",
    "!mv seq2seq.md README.md"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
